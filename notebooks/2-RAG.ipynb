{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Document and Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Preprocessing: PDF to Text Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pdfplumber`: Plumb a PDF for detailed information about each text character, rectangle, and line. Plus: Table extraction and visual debugging.\n",
    "\n",
    "https://github.com/jsvine/pdfplumber\n",
    "\n",
    "Several other Python libraries help users to extract information from PDFs. As a broad overview, pdfplumber distinguishes itself from other PDF processing libraries by combining these features:\n",
    "\n",
    "* Easy access to detailed information about each PDF object\n",
    "* Higher-level, customizable methods for extracting text and tables\n",
    "* Tightly integrated visual debugging\n",
    "* Other useful utility functions, such as filtering objects via a crop-box\n",
    "\n",
    "It's also helpful to know what features pdfplumber does not provide:\n",
    "\n",
    "* PDF generation\n",
    "* PDF modification\n",
    "* Optical character recognition (OCR)\n",
    "* Strong support for extracting tables from OCR'ed documents\n",
    "\n",
    "**Specific comparisons**\n",
    "\n",
    "* `pdfminer.six` provides the foundation for pdfplumber. It primarily focuses on parsing PDFs, analyzing PDF layouts and object positioning, and extracting text. It does not provide tools for table extraction or visual debugging.\n",
    "* `PyPDF2` is a pure-Python library \"capable of splitting, merging, cropping, and transforming the pages of PDF files. It can also add custom data, viewing options, and passwords to PDF files.\" It can extract page text, but does not provide easy access to shape objects (rectangles, lines, etc.), table-extraction, or visually debugging tools.\n",
    "* `pymupdf` is substantially faster than pdfminer.six (and thus also pdfplumber) and can generate and modify PDFs, but the library requires installation of non-Python software (MuPDF). It also does not enable easy access to shape objects (rectangles, lines, etc.), and does not provide table-extraction or visual debugging tools.\n",
    "* `camelot`, `tabula-py`, and `pdftables` all focus primarily on extracting tables. In some cases, they may be better suited to the particular tables you are trying to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from pathlib import Path\n",
    "\n",
    "file_path = Path(\"fis_issue22-3.pdf\").resolve()\n",
    "pdf = pdfplumber.open(file_path)\n",
    "pages = pdf.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pages[0]\n",
    "print(f'page witdh x height = {page.width} x {page.height}')\n",
    "print('[Extracted Text]')\n",
    "print(page.extract_text())\n",
    "page.to_image(resolution=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = pages[4]\n",
    "print(f'page witdh x height = {page.width} x {page.height}')\n",
    "page.to_image(resolution=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding box of `pdfplumber.page.Page` = (`x0`, `top`, `x1`, `bottom`). If `relative` is `True`:\n",
    "* (`x0`, `top`) = (x, y) coordinates of the top-left corner of the box\n",
    "* (`x1`, `bottom`) = (x, y) coordinates of the bottom-right corner of the box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = page.crop((0.0, 0.0, 0.5*page.width, page.height), relative=True, strict=True)\n",
    "right = page.crop((0.5*page.width, 0.0, page.width, page.height), relative=True, strict=True)\n",
    "left.to_image(resolution=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# remove un used strings like \u0007\n",
    "def clean_text(s: str) -> str:\n",
    "    # remove un used strings like \u0007\n",
    "    s = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', ' ', s)\n",
    "    # if three white spaces or more(except '\\n'), replace with empty string\n",
    "    s = re.sub(r' {3,}', '', s)\n",
    "    return s\n",
    "\n",
    "print(clean_text(left.extract_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Layout-Parser` is a unified toolkit for deep learning based document image analysis\n",
    "\n",
    "* https://layout-parser.github.io\n",
    "* https://github.com/Layout-Parser/layout-parser\n",
    "* https://github.com/Layout-Parser/layout-parser/blob/main/examples/Deep%20Layout%20Parsing.ipynb\n",
    "* https://tesseract-ocr.github.io/tessdoc/Installation.html\n",
    "* https://github.com/tesseract-ocr/tesseract\n",
    "* https://yunwoong.tistory.com/51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import layoutparser as lp\n",
    "import torchvision.ops.boxes as bops\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from pdfplumber.page import Page, CroppedPage\n",
    "from layoutparser.models.detectron2 import Detectron2LayoutModel\n",
    "\n",
    "def inference_page(p: Page | CroppedPage, model: Detectron2LayoutModel) -> tuple:\n",
    "    img = p.to_image()\n",
    "    imgfile = BytesIO()\n",
    "    img.save(imgfile, format='png', quantize=False)\n",
    "    img = np.array(Image.open(imgfile).convert('RGB'))\n",
    "    blocks = model.detect(img)\n",
    "    return blocks, img\n",
    "\n",
    "# lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config\n",
    "# lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config\n",
    "model = Detectron2LayoutModel(\n",
    "    'lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config', \n",
    "    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8],\n",
    "    label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3: \"Table\", 4: \"Figure\"}\n",
    ")\n",
    "\n",
    "# using deep learning model to detect the layout of figures\n",
    "blocks, img = inference_page(left, model)\n",
    "lp.draw_box(img, blocks, box_width=3, box_alpha=0.2, show_element_type=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns the coordinates of (`x_0`, `y_0`, `x_1`, `y_1`) for each text box in the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_block_with_coords(block, img):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "    ax.imshow(img)\n",
    "    # draw bounding box of the first block on the img\n",
    "    ax.add_patch(plt.Rectangle(\n",
    "    (block.x_1, block.y_1),   # x, y\n",
    "    block.x_2 - block.x_1,  # width\n",
    "    block.y_2 - block.y_1,  # height\n",
    "    fill=False, edgecolor='red', lw=1.5))\n",
    "    # draw the red dot at the top-left corner of the bounding box\n",
    "    ax.plot(block.x_1, block.y_1, 'ro')\n",
    "    # draw the red dot at the bottom-right corner of the bounding box\n",
    "    ax.plot(block.x_2, block.y_2, 'ro')\n",
    "    # write the coordinates\n",
    "    ax.text(block.x_1 - 10, block.y_1 - 8.5, \n",
    "            f'(x1,y1)=({block.x_1:.2f}, {block.y_1:.2f})', \n",
    "            fontsize=12, color='red')\n",
    "    ax.text(block.x_2 - 100, block.y_2 + 30, \n",
    "            f'(x2,y2)=({block.x_2:.2f}, {block.y_2:.2f})', \n",
    "            fontsize=12, color='red')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.show()\n",
    "\n",
    "block = blocks[0].block\n",
    "print(f'Coordinates of the first block:')\n",
    "print(f'x1: {block.x_1:.4f}, y1: {block.y_1:.4f}')\n",
    "print(f'x2: {block.x_2:.4f}, y2: {block.y_2:.4f}')\n",
    "\n",
    "draw_block_with_coords(block, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the pdf pages for further usage\n",
    "PDF_PAGES = []\n",
    "for page in pages:\n",
    "    if page.height > page.width:\n",
    "        PDF_PAGES.append(page)\n",
    "    else:\n",
    "        left = page.crop((0.0, 0.0, 0.5*page.width, page.height), relative=True, strict=True)\n",
    "        right = page.crop((0.5*page.width, 0.0, page.width, page.height), relative=True, strict=True)\n",
    "        PDF_PAGES.append(left)\n",
    "        PDF_PAGES.append(right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Load Documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Document` is a basic class of object to represent a piece of information. \n",
    "\n",
    "* `page_content`, `metadata`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "doc = Document(\n",
    "    page_content='I am a page content',\n",
    "    metadata = {'page': 0}\n",
    ")\n",
    "doc.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./fis_issue22_3.json', 'r') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "docs = []\n",
    "for data in all_data:\n",
    "    doc = Document(\n",
    "        page_content = data['page_content'],\n",
    "        metadata = data['metadata']\n",
    "    )\n",
    "    docs.append(doc)\n",
    "\n",
    "print(docs[0].page_content)\n",
    "print(f'\\nTotal Documents: {len(docs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benefits for splitting documents:\n",
    "\n",
    "* **Efficient resouce management**: Inputting an entire document into an LLM is costly and hinders efficient answer extraction from extensive information, sometimes causing hallucination issues. Therefore, the aim is to extract only the information needed for the response.\n",
    "* **Accurate information retrieval**: Segmenting documents aids in extracting only the relevant information for a given query. By focusing on specific topics or content within each segment, it ensures the retrieval of highly pertinent information.\n",
    "\n",
    "At a high level, text splitters work as following:\n",
    "\n",
    "1. Split the text up into small, semantically meaningful chunks (often sentences).\n",
    "2. Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
    "3. Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
    "\n",
    "That means there are two different axes along which you can customize your text splitter:\n",
    "\n",
    "1. How the text is split\n",
    "2. How the chunk size is measured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # the maximum size of the split text chunks.\n",
    "    chunk_size=250,\n",
    "    # the number of overlapping characters between split text chunks.\n",
    "    chunk_overlap=0,\n",
    "    # the function to calculate the length of the text.\n",
    "    length_function=len,\n",
    "    # whether the delimiter is a regular expression.\n",
    "    is_separator_regex=False,\n",
    "    \n",
    ")\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "for data in all_data:\n",
    "    texts.append(data['page_content'])\n",
    "    metadatas.append(data['metadata'])\n",
    "documents = text_splitter.create_documents(texts, metadatas)\n",
    "\n",
    "print(f'Original number of pages: {len(all_data)}')\n",
    "print(f'Total Documents: {len(documents)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RecursiveCharacterTextSplitter` is designed for general text and operates by using a specified list of characters as parameters. It splits the text in the order of the provided characters until the resulting chunks are small enough. By default, it uses the characters `[\"\\n\\n\", \"\\n\", \" \", \"\"]`, splitting the text recursively from paragraphs to sentences to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data[4]['page_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_documents = [d for d in documents if d.metadata['page'] == 4]\n",
    "for i, sd in enumerate(split_documents):\n",
    "    print(f'\\n[Split Documents for Page 4] - {i+1}')\n",
    "    print(sd.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding models create a vector representation of a piece of text that captures the semantic meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    api_key = os.getenv('OPENAI_API_KEY'),\n",
    "    model = 'text-embedding-3-small', # text-embedding-ada-002\n",
    "    dimensions = 1024  # 1536 for text-embedding-ada-002\n",
    ")\n",
    "\n",
    "model_name = 'intfloat/multilingual-e5-large-instruct'\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'cpu'},  # cuda, cpu\n",
    "    encode_kwargs={'normalize_embeddings': True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity calculation\n",
    "\n",
    "* `Cosine similarity` is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude.\n",
    "\n",
    "$$\\text{cosine\\_similarity}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\vert\\mathbf{a}\\vert \\vert\\mathbf{b}\\vert} = \\frac{\\sum_{i=1}^{n} a_i b_i}{\\sqrt{\\sum_{i=1}^{n} a_i^2} \\sqrt{\\sum_{i=1}^{n} b_i^2}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the text into a vector \n",
    "e1_en = np.array(openai_embeddings.embed_query('Hello, world!'))\n",
    "e1_kr = np.array(openai_embeddings.embed_query('안녕하세요, 세계!'))\n",
    "e2_en = np.array(hf_embeddings.embed_query('Hello, world!'))\n",
    "e2_kr = np.array(hf_embeddings.embed_query('안녕하세요, 세계!'))\n",
    "\n",
    "# they are not quite similar since it is mostly trained on English\n",
    "print(f'OpenAI Similarity: {e1_en @ e1_kr: .4f}')\n",
    "# they are similar since it is trained on multi-lingual\n",
    "print(f'HuggingFace Similarity: {e2_en @ e2_kr: .4f}')\n",
    "# they are not similar, since they are trained on different models\n",
    "print(f'Two Embeddings Similarity: {e1_en @ e2_en: .4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the embeddings\n",
    "n = 100\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 6), sharex=True)\n",
    "es = np.array([e1_en, e1_kr, e2_en, e2_kr])[:, :n]\n",
    "titles = ['OpenAI(EN)', 'OpenAI(KR)','HuggingFace(EN)', 'HuggingFace(KR)']\n",
    "m = ax.matshow(es, cmap='coolwarm', interpolation='nearest', aspect=4)\n",
    "ax.set_yticks(np.arange(len(titles)))\n",
    "ax.set_yticklabels(titles)\n",
    "ax.set_title('Embeddings (First 100 dimensions)', y=1.1, fontsize=16)\n",
    "ax.set_xticks(list(range(0, n, 10))+[n-1])\n",
    "ax.set_xticklabels([1] + list(range(10, n, 10))+[n])\n",
    "ax.set_xlabel('n-th Dimension')\n",
    "plt.gca().xaxis.tick_bottom()\n",
    "plt.colorbar(m, orientation='vertical', fraction=0.35, shrink=0.40, pad=0.05, label='normalized value', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Vector Store and Retriever\n",
    "\n",
    "A vector store takes care of storing embedded data and performing vector searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS \n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "faiss_db = FAISS.from_documents(\n",
    "    documents,     # embedding documents\n",
    "    hf_embeddings  # embedding model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`FAISS` is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. FAISS is written in C++ with complete wrappers for Python/numpy. Some of the most useful algorithms are implemented on the GPU. It is developed by Facebook AI Research.\n",
    "\n",
    "* https://github.com/facebookresearch/faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity search with vector store\n",
    "similar_docs = faiss_db.similarity_search('재정 융자사업의 개념은 무엇인가요?', k=3)\n",
    "\n",
    "# returns the list of similar documents with the score(L2 distance)\n",
    "similar_docs = faiss_db.similarity_search_with_score('재정융자 사업의 개념은 무엇인가요?', k=3)\n",
    "\n",
    "print('[Search Results]')\n",
    "for i, (doc, score) in enumerate(similar_docs):\n",
    "    print(f'Document {i}(s={score:.4f}) - Page: {doc.metadata[\"page\"]}')\n",
    "    print(f'{doc.page_content}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PAGES[2].to_image(resolution=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the documents with the metadata\n",
    "similar_docs = faiss_db.similarity_search_with_score(\n",
    "    '재정융자 사업의 개념은 무엇인가요?', k=3, filter=dict(page=10)\n",
    ")\n",
    "\n",
    "print('[Search Results]')\n",
    "for i, (doc, score) in enumerate(similar_docs):\n",
    "    print(f'Document {i}(s={score:.4f}) - Page: {doc.metadata[\"page\"]}')\n",
    "    print(f'{doc.page_content}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PAGES[10].to_image(resolution=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. \n",
    "\n",
    "The `MMR (Maximal Marginal Relevance)` method avoids redundancy in query results by balancing relevance and diversity. Instead of retrieving only the most relevant items, MMR ensures that the results include diverse perspectives and new information, preventing the selection of very similar items. This approach helps users gain a broader understanding of a topic by considering both the relevance of documents to the query and their dissimilarity from already selected documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity search with retriever\n",
    "# we can use LCEL style query\n",
    "retriever = faiss_db.as_retriever(\n",
    "    search_type = 'mmr',    # \"similarity\", \"mmr\", or \"similarity_score_threshold\".\n",
    "    search_kwargs={'k': 3}  # {\"score_threshold\": 0.75}\n",
    ")\n",
    "similar_docs = retriever.invoke('재정 융자사업의 개념은 무엇인가요?')\n",
    "print('[Search Results]')\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f'Document {i}(s={score:.4f}) - Page: {doc.metadata[\"page\"]}')\n",
    "    print(f'{doc.page_content}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also other retriever classes, such as `EnsembleRetriever`, `SelfQueryRetriever`\n",
    "\n",
    "* `EnsembleRetriever`: An ensemble of retrievers that aggregates their results.\n",
    "* `SelfQueryRetriever`: A retriever that generate question by itself with LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Naive Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lh3.googleusercontent.com/d/1J7JasPNjGfAySxvZoqG7aV29F1nYuxXQ\" width=\"75%\">\n",
    "\n",
    "1. **Indexing**: Documents are split into chunks, encoded into vectors, and stored in a vector database. (Relevant to `VectorStore`)\n",
    "2. **Retrieval**: Retrieve the Top k chunks most relevant to the question based on semantic similarity. (Relevant to `Retriever`)\n",
    "3. **Generation**: Input the original question and the retrieved chunks together into LLM to generate the final answer. (Relevant to `LCEL pipeline`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Step 1: Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "with open('./fis_issue22_3.json', 'r') as f:\n",
    "    all_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Step 2: Split documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, chunk_overlap=50, length_function=len,\n",
    ")\n",
    "\n",
    "texts = []\n",
    "for data in all_data:\n",
    "    texts.append(data['page_content'])\n",
    "texts = '\\n\\n'.join(texts)\n",
    "documents = text_splitter.create_documents([texts])\n",
    "\n",
    "print(f'Original number of pages: {len(all_data)}')\n",
    "print(f'Total Documents: {len(documents)}')\n",
    "print(texts.split('\\n\\n')[1][:767])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lh3.googleusercontent.com/d/1JCH8jNIWbWFyPb2bEaFkgMYrP1R5AB7w\" width=\"50%\">\n",
    "\n",
    "visualize chunks: https://chunkviz.up.railway.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Step 3: Create embeddings and store them in a vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(api_key = os.getenv('OPENAI_API_KEY'))\n",
    "faiss_db = FAISS.from_documents(documents, openai_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Step 4: Create a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = faiss_db.as_retriever(\n",
    "    search_type='similarity_score_threshold',  # \"similarity\", \"mmr\", or \"similarity_score_threshold\".\n",
    "    search_kwargs={'k': 3, 'score_threshold': 0.75}\n",
    ")\n",
    "\n",
    "similar_docs = retriever.invoke('재정 융자사업의 개념은 무엇인가요?')\n",
    "print('[Search Results]')\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f'Document {i}')\n",
    "    print(f'{doc.page_content}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 Step 5: Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = '''You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "When you answer the question you must quote the context number and the relevant text.\n",
    "The output format should be a JSON object with the following structure:\n",
    "```json\n",
    "{{\n",
    "    \"context_id\": \"The retrieved context number(integer)\",\n",
    "    \"quote\": \"The relevant text(part of the context)\",\n",
    "    \"answer\": \"The answer to the question\"\n",
    "}}\n",
    "```\n",
    "Answer in Korean.\n",
    "\n",
    "## Question:\n",
    "{question}\n",
    "\n",
    "## Context:\n",
    "{context}\n",
    "\n",
    "## Answer:\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=['question', 'context']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.6 Step 6: Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',  # latest model\n",
    "    temperature=0.5,\n",
    "    model_kwargs = {'response_format': {'type': 'json_object'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.7 Step 7: Create Chain and Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    context_id: int = Field(..., title='The retrieved context number(integer)')\n",
    "    quote: str = Field(..., title='The relevant text(part of the context)')\n",
    "    answer: str = Field(..., title='The answer to the question')\n",
    "\n",
    "def numbering_context(context: list) -> str:\n",
    "    s = ''\n",
    "    for i, c in enumerate(context):\n",
    "        s += f'[source {i+1}] {c}\\n'\n",
    "    return s\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "\n",
    "chain = (\n",
    "    {'question': RunnablePassthrough(), 'context': retriever | RunnableLambda(numbering_context)}\n",
    "    | prompt\n",
    "    | model_openai\n",
    "    | JsonOutputParser(pydantic_object=Answer)\n",
    ")\n",
    "\n",
    "question = '1990년대 이전의 재정 융자사업은 어떻게 진행되었나요?'\n",
    "answer = chain.invoke(question)\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check LangSmith: https://smith.langchain.com/public/377158a0-fc0b-42a7-8a1f-34a39943f949/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "print(f'Referred Context: {answer[\"context_id\"]}')\n",
    "pprint(f'Quote: {answer[\"quote\"]}')\n",
    "print()\n",
    "pprint(answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Application Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario: Travel Enthusiast's Quest for a Perfect Vacation\n",
    "\n",
    "Meet Alex, an avid traveler planning an unforgettable trip to three of the world's most iconic cities: New York, Paris, and Seoul. With a passion for discovering hidden gems and indulging in local cuisines, Alex turns to the innovative Travel Search Guide powered by Retrieval-Augmented Generation (RAG). Alex begins by entering preferences into the guide—luxurious hotels, must-see attractions, and top-rated restaurants. The guide instantly retrieves and presents detailed options from a curated database, including renowned hotels like The Ritz in Paris and The Shilla in Seoul, attractions such as Central Park and the Eiffel Tower, and culinary delights from Le Bernardin in New York to Tosokchon Samgyetang in Seoul. With comprehensive information at Alex’s fingertips, including ratings, contact details, and accessibility options, planning the perfect itinerary becomes effortless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./travel_guides_db.txt', 'r') as f:\n",
    "    tg_data = f.read().split('\\n\\n')\n",
    "\n",
    "print(f'Total records: {len(tg_data)}\\n')\n",
    "print(tg_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from pprint import pprint\n",
    "docs = []\n",
    "for i, data in enumerate(tg_data):\n",
    "    name_and_type = data.split('###')[0].lstrip('## ').rstrip()\n",
    "    typ, name = name_and_type.split(': ')\n",
    "    doc = Document(\n",
    "        page_content = data,\n",
    "        metadata = {'id': i, 'type': typ, 'name': name}\n",
    "    )\n",
    "    docs.append(doc)\n",
    "\n",
    "list_of_availables = [d.metadata['name'] for d in docs]\n",
    "pprint(list_of_availables[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Create Embeddings, Vector Store, and Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules['pysqlite3']\n",
    "\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model for answer format\n",
    "class Answer(BaseModel):\n",
    "    context_id: list[int] = Field(..., title='The retrieved context number(list of integer) e.g., [1, 3]')\n",
    "    quote: list[str] = Field(..., title=\"The relevant text(part of the context, list of strings), e.g., ['quote from context_id=1', 'quote from context_id=3']\")\n",
    "    answer: str = Field(..., title='The final answer to the question')\n",
    "\n",
    "# pipeline formatting context with numbering\n",
    "def numbering_context(context: list) -> str:\n",
    "    s = ''\n",
    "    for i, c in enumerate(context):\n",
    "        s += f'[source {i+1}] {c}\\n'\n",
    "    return s\n",
    "\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "# define LLM model for self-query retriever\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.5,\n",
    ")\n",
    "# create embeddings and vector store\n",
    "openai_embeddings = OpenAIEmbeddings(api_key = os.getenv('OPENAI_API_KEY'))\n",
    "vectorstore = Chroma.from_documents(docs, embedding=openai_embeddings)\n",
    "\n",
    "# create self-query retriever\n",
    "metadata_fields = [\n",
    "    AttributeInfo(name='id', type='int', description='The unique identifier of the document'),\n",
    "    AttributeInfo(name='type', type='str', description='The type of the document: one of [\"Hotel\", \"Restaurant\", \"Attraction\"]'),\n",
    "]\n",
    "contents_description = ''''The documents about three cities in New York, Paris and Seoul. \n",
    "We have three types of documents: Hotel, Restaurant, and Attraction.'''\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vectorstore,\n",
    "    document_contents=contents_description,\n",
    "    metadata_field_info=metadata_fields,\n",
    "    search_type='similarity_score_threshold',\n",
    "    search_kwargs={'k': 5, 'score_threshold': 0.65}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filterout the documents with the metadata\n",
    "def remove_duplicates_by_id(documents: list) -> list:\n",
    "    seen = set()\n",
    "    return [d for d in documents if not (d.metadata['id'] in seen or seen.add(d.metadata['id']))]\n",
    "\n",
    "documents = retriever.invoke(\n",
    "    'What activities and highlights can visitors enjoy at the tower in the mountain of Seoul?'\n",
    ")\n",
    "print(f'Before removing duplicates: {len(documents)}')\n",
    "documents = remove_duplicates_by_id(documents)\n",
    "print(f'After removing duplicates: {len(documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = '''You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "When you answer the question you must quote the context number and the relevant text.\n",
    "The output format should be a JSON object with the following structure:\n",
    "```json\n",
    "{{\n",
    "    \"context_id\": \"The retrieved context number(list of integer) e.g., [1, 3]\",\n",
    "    \"quote\": \"The relevant text(part of the context, list of strings), e.g., ['quote from context_id=1', 'quote from context_id=3']\"\n",
    "    \"answer\": \"The final answer to the question\"\n",
    "}}\n",
    "```\n",
    "\n",
    "[Question]:\n",
    "{question}\n",
    "\n",
    "[Context]:\n",
    "{context}\n",
    "\n",
    "[Answer]:\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=['question', 'context']\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0.5,\n",
    "    model_kwargs = {'response_format': {'type': 'json_object'}}\n",
    ")\n",
    "\n",
    "# TODO: Implement the pipeline\n",
    "chain = (\n",
    "    {'question': RunnablePassthrough(), \n",
    "     'context': retriever | RunnableLambda(remove_duplicates_by_id) | RunnableLambda(numbering_context)}\n",
    "    | prompt\n",
    "    | model_openai\n",
    "    | JsonOutputParser(pydantic_object=Answer)\n",
    ")\n",
    "\n",
    "question = 'What activities and highlights can visitors enjoy at the tower in the mountain of Seoul?'\n",
    "\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "answer = chain.invoke(question)\n",
    "# os.environ['LANGCHAIN_TRACING_V2'] = 'false'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the detail process in LangSmith: https://smith.langchain.com/public/8c10b8f4-adf9-43ae-879b-272b2bb0daad/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Referred Context: {answer[\"context_id\"]}')\n",
    "pprint(f'Quote: {answer[\"quote\"]}')\n",
    "print()\n",
    "pprint(answer['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-app-sample-q-rYsYuW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
