{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikidocs.net/book/14314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 LangChain Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Options:\n",
    "* model_name(model): the name of the model\n",
    "* temperature: 0 ~ 2, larger is more random\n",
    "* max_tokens: depends on the model, the maximum number of tokens to generate\n",
    "* base_url: the base URL of the API\n",
    "* api_key: the API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Document: https://platform.openai.com/docs/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',  # latest model\n",
    "    temperature=0.2,\n",
    "    max_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Ollama - Open Source LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama local\n",
    "* model options: `~/.ollama/models` (Mac) or `/usr/share/ollama/.ollama/models` (Linux)\n",
    "* `gemma2:9b-instruct-q4_0`: 9 billion parameters with instruct tuning and 4-bit quantization\n",
    "* modelfile: A model file is the blueprint to create and share models with Ollama.\n",
    "     \n",
    "    ```shell\n",
    "    FROM $HOME/.ollama/models/blobs/sha256-aqoqkj13ik4ujn9oducnzkjdn\n",
    "    TEMPLATE \"<start_of_turn>user\n",
    "    {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }}<end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    {{ .Response }}<end_of_turn>\n",
    "    \"\n",
    "    PARAMETER stop <start_of_turn>\n",
    "    PARAMETER stop <end_of_turn>\n",
    "    ```\n",
    "    \n",
    "    * `PARAMETER`: \n",
    "        * temperature: 0 ~ 2, higher is more creative, lower is more coherent\n",
    "        * num_ctx: context window size\n",
    "    * `SYSTEM`: custom system message \n",
    "* Ollama CoLab\n",
    "* Ollama LangChain: must install `langhain-ollama` seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrok URL: https://square-sunfish-vaguely.ngrok-free.app\n",
      "LangChain Tracing: false\n",
      "LangChain Project Name: sample_app\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Ngrok URL: {os.getenv('NGROK_URL')}\")\n",
    "print(f\"LangChain Tracing: {os.getenv('LANGCHAIN_TRACING_V2')}\")\n",
    "print(f\"LangChain Project Name: {os.getenv('LANGCHAIN_PROJECT')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma:2b-instruct', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LECL: LangChain Expression Language "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain use `Runnable` interface to chain together a series of operations. Any two runnables can be 'chained' together into sequences. We can use `.pipe` method or `|` operator to chain operations together. The output of the previous runnable's `.invoke()` call is passed as input to the next runnable. \n",
    "\n",
    "* `invoke`: call the chain of operations\n",
    "* `stream`: stream the output of the chain of operations\n",
    "* `batch`: call the chain of operations with batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Type of `chain` object: <class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "# 1 `chain_openai`\n",
      "1 + 1 equals 2.\n",
      "# 2 `chain_pipe_ollama`\n",
      "1 + 1 = 2.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template('What is {topic}? Please answer in one sentence.')\n",
    "\n",
    "chain_openai = (prompt | model_openai | StrOutputParser())\n",
    "chain_pipe_ollama = (prompt.pipe(model_ollama).pipe(StrOutputParser()))\n",
    "\n",
    "print('Class Type of `chain` object:', type(chain_openai))\n",
    "print('# 1 `chain_openai`')\n",
    "print(chain_openai.invoke(input={'topic': '1 + 1'}))\n",
    "print('# 2 `chain_pipe_ollama`')\n",
    "print(chain_pipe_ollama.invoke(input={'topic': '1 + 1'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful `Runnable` classes:\n",
    "\n",
    "* `RunnablePassthrough` is a special runnable that passes the input to the output without any modification.\n",
    "    * if call `.assign` method, the output of the chain of operations is assigned to the variable.\n",
    "* `RunnableLambda` can be used to run a user-defined function on the input.\n",
    "* `RunnableParallel` is useful for running multiple runnables in parallel, but can also be useful for manipulating the output of one `Runnable` to match the input format of the next `Runnable` in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"A\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"A\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "RunnablePassthrough:\n",
      "   text='What is two words that start with A?'\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
    "\n",
    "set_debug(True)\n",
    "set_verbose(False)\n",
    "prompt = PromptTemplate.from_template('What is two words that start with {letter}?')\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | prompt\n",
    ")\n",
    "print('RunnablePassthrough:\\n  ', chain.invoke(input='A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter> > chain:RunnableParallel<letter>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter> > chain:RunnableParallel<letter> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter> > chain:RunnableParallel<letter> > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter> > chain:RunnableParallel<letter>] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"A\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [6ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "RunnablePassthrough with assign:\n",
      "   text='What is two words that start with A?'\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough().assign(letter=lambda x: x['letter'].upper())\n",
    "    | prompt\n",
    ")\n",
    "print('RunnablePassthrough with assign:\\n  ', chain.invoke(input={'letter': 'a'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"A\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "RunnableLambda:\n",
      "   text='What is two words that start with A?'\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    RunnableLambda(lambda x: x['letter'].upper()) \n",
    "    | prompt\n",
    ")\n",
    "print('RunnableLambda:\\n  ', chain.invoke(input={'letter': 'a'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-------------+      \n",
      "    | LambdaInput |      \n",
      "    +-------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "       +--------+        \n",
      "       | Lambda |        \n",
      "       +--------+        \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "   +----------------+    \n",
      "   | PromptTemplate |    \n",
      "   +----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "+----------------------+ \n",
      "| PromptTemplateOutput | \n",
      "+----------------------+ \n"
     ]
    }
   ],
   "source": [
    "set_debug(False)\n",
    "set_verbose(False)\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'letter': 'a'}, 'extra': 'A', 'modefied': 'K'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "chain = RunnableParallel(\n",
    "    passed = RunnablePassthrough(),\n",
    "    extra = RunnableLambda(lambda x: x['letter'].upper()),\n",
    "    modefied = lambda x: random.choice(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\n",
    ")\n",
    "chain.invoke(input={'letter': 'a'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "\n",
      "{'end': 'Here are two words that end with the letter \"a\": \\n'\n",
      "        '\\n'\n",
      "        '1. Arena\\n'\n",
      "        '2. Panda',\n",
      " 'start': 'Here are two words that start with the letter \"A\": \\n'\n",
      "          '\\n'\n",
      "          '1. Apple\\n'\n",
      "          '2. Adventure'}\n",
      "Graph:\n",
      "            +--------------------------+             \n",
      "            | Parallel<start,end>Input |             \n",
      "            +--------------------------+             \n",
      "                 **               **                 \n",
      "              ***                   ***              \n",
      "            **                         **            \n",
      "  +-------------+                  +-------------+   \n",
      "  | Passthrough |                  | Passthrough |   \n",
      "  +-------------+                  +-------------+   \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "+----------------+                +----------------+ \n",
      "| PromptTemplate |                | PromptTemplate | \n",
      "+----------------+                +----------------+ \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "  +------------+                    +------------+   \n",
      "  | ChatOpenAI |                    | ChatOpenAI |   \n",
      "  +------------+                    +------------+   \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "+-----------------+              +-----------------+ \n",
      "| StrOutputParser |              | StrOutputParser | \n",
      "+-----------------+              +-----------------+ \n",
      "                 **               **                 \n",
      "                   ***         ***                   \n",
      "                      **     **                      \n",
      "            +---------------------------+            \n",
      "            | Parallel<start,end>Output |            \n",
      "            +---------------------------+            \n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "chain1 = (\n",
    "    RunnablePassthrough()\n",
    "    | PromptTemplate.from_template('What is two words that start with {letter}?')\n",
    "    | model_openai\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain2 = (\n",
    "    RunnablePassthrough()\n",
    "    | PromptTemplate.from_template('What is two words that ends with {letter}?')\n",
    "    | model_openai\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain = RunnableParallel(\n",
    "    start=chain1,\n",
    "    end=chain2\n",
    ")\n",
    "output = chain.invoke(input={'letter': 'a'})\n",
    "print('Output:\\n')\n",
    "pprint(output)\n",
    "print('Graph:')\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "```python\n",
    "# travel information searching\n",
    "inputs = {'country': 'Korea', 'city': 'Jeonju'}\n",
    "# chain 1: find the two famous restaurant in the city\n",
    "# chain 2: find the three famous tourist spot in the city\n",
    "# chain 3: summarize the information in one paragraph chain 1 and chain 2\n",
    "```\n",
    "\n",
    "Total Graph\n",
    "\n",
    "```plaintext\n",
    "              +----------------------+               \n",
    "              | Parallel<c1,c2>Input |               \n",
    "              +----------------------+               \n",
    "                 **               **                 \n",
    "            **                         **            \n",
    "  +-------------+                  +-------------+   \n",
    "  | Passthrough |                  | Passthrough |   \n",
    "  +-------------+                  +-------------+   \n",
    "          *                               *          \n",
    "+----------------+                +----------------+ \n",
    "| PromptTemplate |                | PromptTemplate | \n",
    "+----------------+                +----------------+ \n",
    "          *                               *          \n",
    "  +------------+                    +------------+   \n",
    "  | ChatOllama |                    | ChatOllama |   \n",
    "  +------------+                    +------------+   \n",
    "          *                               *          \n",
    "+-----------------+              +-----------------+ \n",
    "| StrOutputParser |              | StrOutputParser | \n",
    "+-----------------+              +-----------------+ \n",
    "                 **               **                 \n",
    "                      **     **                      \n",
    "              +-----------------------+              \n",
    "              | Parallel<c1,c2>Output |              \n",
    "              +-----------------------+              \n",
    "                          *                          \n",
    "                 +----------------+                  \n",
    "                 | PromptTemplate |                  \n",
    "                 +----------------+                  \n",
    "                          *                          \n",
    "                   +------------+                    \n",
    "                   | ChatOllama |                    \n",
    "                   +------------+                    \n",
    "                          *                          \n",
    "                +-----------------+                  \n",
    "                | StrOutputParser |                  \n",
    "                +-----------------+                  \n",
    "                          *                          \n",
    "              +-----------------------+              \n",
    "              | StrOutputParserOutput |              \n",
    "              +-----------------------+    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',  # latest model\n",
    "    temperature=0.2,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL')\n",
    ")\n",
    "\n",
    "model = model_ollama\n",
    "chain1 = (\n",
    "    # a1: RunnablePassthrough or itemgetter\n",
    "    # a2: PromptTemplate.from_template(  ?  )\n",
    "    # | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "chain2 = (\n",
    "    # b1: RunnablePassthrough or itemgetter\n",
    "    # b2: PromptTemplate.from_template(  ?  )\n",
    "    # | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "chain = (\n",
    "    # RunnableParallel(c1=chain1, c2=chain2) \n",
    "    # c1: PromptTemplate.from_template(  ?  )\n",
    "    # | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "output = chain.invoke(input= {'country': 'Korea', 'city': 'Jeonju'})\n",
    "print('Output:\\n')\n",
    "print(output)\n",
    "print('Graph:')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **prompt** is natural language text describing the task that an AI should perform. It can contain information like the instruction or question passing to the model and include other details such as context, inputs, or examples. We can use these elements to instruct the model better and get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Prompt Template\n",
    "\n",
    "`PromptTemplate` helps to translate user input and parameters into instructions for a language model.\n",
    "* input: a dictionary of parameters\n",
    "* output: `PromptValue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt1: text='What is Korea capital city?'\n",
      "prompt2: text='What is Korea capital city?'\n",
      "prompt3: text='Date: 2024-08-05\\nWhat is Korea capital city?'\n",
      "\"Input to PromptTemplate is missing variables {'country'}.  Expected: ['country'] Received: ['c']\"\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate\n",
    "from datetime import datetime as dt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = 'What is {country} capital city?'\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(template)\n",
    "prompt2 = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=['country']\n",
    ")\n",
    "prompt3 = PromptTemplate(\n",
    "    template='Date: {today}\\n' + template, \n",
    "    input_variables=['country'], \n",
    "    partial_variables={'today': dt.now().strftime('%Y-%m-%d')}  # predefined partial values\n",
    ")\n",
    "\n",
    "print('prompt1:', prompt1.invoke({'country': 'Korea'}))\n",
    "print('prompt2:', prompt2.invoke({'country': 'Korea'}))\n",
    "print('prompt3:', prompt3.invoke({'country': 'Korea'}))\n",
    "try:\n",
    "    prompt2.invoke({'c': 'Korea'})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate` is used to format a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt1\n",
      "messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='Tell me a joke about cats')]\n",
      "\n",
      "prompt2\n",
      "messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='hi!')]\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant'),\n",
    "    ('user', 'Tell me a joke about {topic}')\n",
    "])\n",
    "prompt2 = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant'),\n",
    "    MessagesPlaceholder('msgs')\n",
    "])\n",
    "\n",
    "print('prompt1')\n",
    "print(prompt1.invoke({'topic': 'cats'}))\n",
    "print()\n",
    "print('prompt2')\n",
    "print(prompt2.invoke({'msgs': [HumanMessage(content='hi!')]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Few shot prompts\n",
    "\n",
    "One common prompting technique for achieving better performance is to include examples as part of the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Chain1\n",
      "[Prompt]\n",
      "Translate english to Korean.\n",
      "Input: deep learning > Output: \n",
      "[Output]\n",
      "ê¹Šì€ í•™ìŠµ  (kireun haksaep) \n",
      "\n",
      "\n",
      "Let me know if you have any other phrases you'd like translated! ðŸ˜Š \n",
      "\n",
      "========================================\n",
      "# Chain2\n",
      "[Prompt]\n",
      "Translate english to Korean.\n",
      "\n",
      "Input: Hello > Output: ì•ˆë…•\n",
      "\n",
      "Input: Goodbye > Output: ìž˜ê°€\n",
      "\n",
      "Input: Thank you > Output: ê³ ë§ˆì›Œ\n",
      "\n",
      "Input: deep learning > Output:\n",
      "[Output]\n",
      "Input: deep learning > Output: ë”¥ëŸ¬ë‹ \n",
      "\n",
      "\n",
      "Let me know if you'd like to translate anything else! ðŸ˜Š  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL')\n",
    ")\n",
    "# translation: English to Korean\n",
    "examples = [\n",
    "    {'input': 'Hello', 'output': 'ì•ˆë…•'},\n",
    "    {'input': 'Goodbye', 'output': 'ìž˜ê°€'},\n",
    "    {'input': 'Thank you', 'output': 'ê³ ë§ˆì›Œ'}\n",
    "]\n",
    "instruction = 'Translate english to Korean.'\n",
    "example_template = 'Input: {input} > Output: {output}'\n",
    "\n",
    "# zero-shot prompting\n",
    "prompt1 = PromptTemplate(\n",
    "    template=instruction+'\\n'+example_template, \n",
    "    input_variables=['input'], \n",
    "    partial_variables={'output': ''}\n",
    ")\n",
    "chain1 = (\n",
    "    prompt1\n",
    "    | model_ollama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# few-shot prompting\n",
    "prompt2 = FewShotPromptTemplate(\n",
    "    prefix=instruction,\n",
    "    examples=examples,\n",
    "    example_prompt=PromptTemplate.from_template(template=example_template),\n",
    "    suffix='Input: {input} > Output:',\n",
    "    input_variables=['input'],\n",
    ")\n",
    "chain2 = (\n",
    "    prompt2\n",
    "    | model_ollama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print('# Chain1\\n[Prompt]')\n",
    "print(prompt1.invoke(input='deep learning').text)\n",
    "print('[Output]')\n",
    "print(chain1.invoke(input='deep learning'))\n",
    "print('===='*10)\n",
    "print('# Chain2\\n[Prompt]')\n",
    "print(prompt2.invoke(input='deep learning').text)\n",
    "print('[Output]')\n",
    "print(chain2.invoke(input='deep learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n",
    "\n",
    "* Get format instructions: A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "* Parse: A method which takes in a string (assumed to be the response from a language model) and parses it into some structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL'),\n",
    "    num_ctx=500\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt]\n",
      "List the five city of Korea.\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
      "[Output: Gemma2]\n",
      "['Seoul', 'Busan', 'Daegu', 'Incheon', \"Gwangju \\n\\n\\nLet me know if you'd like to know more about these cities! ðŸ˜Š\"]\n",
      "[Output: OpenAI]\n",
      "['Seoul', 'Busan', 'Incheon', 'Daegu', 'Gwangju']\n"
     ]
    }
   ],
   "source": [
    "# Comma Sprerated List\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='List the five city of {country}.\\n{format_instructions}',\n",
    "    input_variables=['country'],\n",
    "    partial_variables={'format_instructions': output_parser.get_format_instructions()}\n",
    ")\n",
    "chain1 = prompt | model_ollama | output_parser\n",
    "chain2 = prompt | model_openai | output_parser\n",
    "\n",
    "print('[Prompt]')\n",
    "print(prompt.invoke(input={'country': 'Korea'}).text)\n",
    "print('[Output: Gemma2]')\n",
    "print(chain1.invoke(input={'country': 'Korea'}))  # gemma2 is too nice\n",
    "print('[Output: OpenAI]')\n",
    "print(chain2.invoke(input={'country': 'Korea'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt]\n",
      "('List two cities in Korea and tell me the population of these two cities.\\n'\n",
      " 'The output should be formatted as a JSON instance that conforms to the JSON '\n",
      " 'schema below.\\n'\n",
      " '\\n'\n",
      " 'As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", '\n",
      " '\"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": '\n",
      " '\"string\"}}}, \"required\": [\"foo\"]}\\n'\n",
      " 'the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the '\n",
      " 'schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not '\n",
      " 'well-formatted.\\n'\n",
      " '\\n'\n",
      " 'Here is the output schema:\\n'\n",
      " '```\\n'\n",
      " '{\"properties\": {\"cities\": {\"title\": \"Cities\", \"description\": \"List of cities '\n",
      " '\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/City\"}}}, \"required\": '\n",
      " '[\"cities\"], \"definitions\": {\"City\": {\"title\": \"City\", \"type\": \"object\", '\n",
      " '\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"City name\", \"type\": '\n",
      " '\"string\"}, \"population\": {\"title\": \"Population\", \"description\": \"City '\n",
      " 'population\", \"type\": \"integer\"}}, \"required\": [\"name\", \"population\"]}}}\\n'\n",
      " '```')\n",
      "\n",
      "[Output: Gemma2]\n",
      "{'cities': [{'name': 'Seoul', 'population': 9600000}, {'name': 'Busan', 'population': 3400000}]}\n",
      "[Output: OpenAI]\n",
      "{'cities': [{'name': 'Seoul', 'population': 9776000}, {'name': 'Busan', 'population': 3406000}]}\n"
     ]
    }
   ],
   "source": [
    "# Json \n",
    "# - Pydantic: data validation library for Python\n",
    "from pprint import pprint\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class City(BaseModel):\n",
    "    name: str = Field(description='City name')\n",
    "    population: int = Field(description='City population')\n",
    "\n",
    "class CityList(BaseModel):\n",
    "    cities: list[City] = Field(description='List of cities ')\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=CityList)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='List two cities in {country} and tell me the population of these two cities.\\n{format_instructions}',\n",
    "    input_variables=['country'],\n",
    "    partial_variables={'format_instructions': output_parser.get_format_instructions()}\n",
    ")\n",
    "chain1 = prompt | model_ollama | output_parser\n",
    "chain2 = prompt | model_openai | output_parser\n",
    "\n",
    "print('[Prompt]')\n",
    "pprint(prompt.invoke(input={'country': 'Korea'}).text)\n",
    "print()\n",
    "print('[Output: Gemma2]')\n",
    "print(chain1.invoke(input={'country': 'Korea'}))\n",
    "print('[Output: OpenAI]')\n",
    "print(chain2.invoke(input={'country': 'Korea'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.with_structured_output()` method is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\n",
    "\n",
    "`WARNINGS`: only support for OpenAI for now.\n",
    "* support list: [https://python.langchain.com/v0.2/docs/integrations/chat/](https://python.langchain.com/v0.2/docs/integrations/chat/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pydantic Object]: <class '__main__.CityList'>\n",
      "cities=[City(name='Seoul', population=9776000), City(name='Busan', population=3406000)]\n",
      "[Json Object]: <class 'dict'>\n",
      "{'cities': [{'name': 'Seoul', 'population': 9776000}, {'name': 'Busan', 'population': 3406000}]}\n"
     ]
    }
   ],
   "source": [
    "# output pydantic object\n",
    "class City(BaseModel):\n",
    "    name: str = Field(description='City name')\n",
    "    population: int = Field(description='City population')\n",
    "\n",
    "class CityList(BaseModel):\n",
    "    cities: list[City] = Field(description='List of cities ')\n",
    "\n",
    "llm = model_openai.with_structured_output(CityList)\n",
    "chain = prompt | llm\n",
    "output = chain.invoke(input={'country': 'Korea'})\n",
    "\n",
    "print(f'[Pydantic Object]: {type(output)}')\n",
    "print(output)\n",
    "\n",
    "# output Json object\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "class City2(TypedDict):\n",
    "    name: Annotated[str, ..., 'City name']\n",
    "    population: Annotated[int, ..., 'City population']\n",
    "\n",
    "class CityList2(TypedDict):\n",
    "    cities: Annotated[list[City2], 'List of cities']\n",
    "\n",
    "llm = model_openai.with_structured_output(CityList2)\n",
    "chain = prompt | llm\n",
    "output = chain.invoke(input={'country': 'Korea'})\n",
    "\n",
    "print(f'[Json Object]: {type(output)}')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output: Gemma2] <class 'str'>\n",
      "{\n",
      "  \"cities\": [\n",
      "    {\n",
      "      \"name\": \"Seoul\",\n",
      "      \"population\": 9600000\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Busan\",\n",
      "      \"population\": 3400000\n",
      "    }\n",
      "  ]\n",
      "} \n",
      "\n",
      "\n",
      "\n",
      "[Output: OpenAI] <class 'str'>\n",
      "\n",
      "{\n",
      "  \"cities\": [\n",
      "    {\n",
      "      \"name\": \"Seoul\",\n",
      "      \"population\": 9746000\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Busan\",\n",
      "      \"population\": 3406000\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# model oriented format output\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b',\n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL'),\n",
    "    num_ctx=500,\n",
    "    format='json'\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    model_kwargs = {'response_format': {'type': 'json_object'}}\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='''List two cities in {country} and tell me the population of these two cities.\n",
    "Return the answer as a list of JSON object. e.g., [{{\"key1\": \"value1\", \"key2\": \"value2\"}}, ...] \n",
    "Each JSON object should have keys and their values: \n",
    "| \"name\" | \"type\": \"string\", \"description\": \"the name of the city\"\n",
    "| \"population\" | \"type\": \"integer\", \"description\": \"the population of the city\"\n",
    "''',\n",
    "    input_variables=['country'],\n",
    "    partial_variables={'schema': CityList.schema()}\n",
    ")\n",
    "chain1 = prompt | model_ollama\n",
    "chain2 = prompt | model_openai\n",
    "o1 = chain1.invoke(input={'country': 'Korea'})\n",
    "o2 = chain2.invoke(input={'country': 'Korea'})\n",
    "# the output of model is AI\n",
    "print(f'[Output: Gemma2] {type(o1.content)}')\n",
    "print(o1.content)\n",
    "print(f'[Output: OpenAI] {type(o2.content)}')\n",
    "print(o2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Chain of Thoughts Prompting\n",
    "\n",
    "Chain of thoughts prompting is a series of intermediate reasoning steps that guide the model to the final answer.\n",
    "\n",
    "`GSM8K (Grade School Math 8K)` is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Question]\n",
      "Janetâ€™s ducks lay 16 eggs per day.\n",
      "She eats three for breakfast every morning and bakes muffins for her friends every day with four.\n",
      "She sells the remainder at the farmers' market daily for $2 per fresh duck egg.\n",
      "How much in dollars does she make every day at the farmers' market?.\n",
      "[Chain of Thought]\n",
      "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmerâ€™s market.\n",
      "[Answer]\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(df: pd.DataFrame) -> list[dict[str, str]]:\n",
    "    df.rename(columns={'answer': 'cot_answer'}, inplace=True)\n",
    "    df['cot'] = df['cot_answer'].apply(lambda x: x.split('####')[0].strip())\n",
    "    df['answer'] = df['cot_answer'].apply(lambda x: x.split('####')[1].strip())\n",
    "    return df.to_dict(orient='records')\n",
    "\n",
    "df = pd.read_csv('GSM8K_test.csv')\n",
    "examples = df.loc[range(0, 5)].copy()\n",
    "queries = df.loc[range(5, 8)].copy()\n",
    "examples = process_data(examples)\n",
    "queries = process_data(queries)\n",
    "\n",
    "print(f'[Question]')\n",
    "for x in examples[0][\"question\"].split(\". \"):\n",
    "    print(x, end='.\\n')\n",
    "print(f'[Chain of Thought]\\n{examples[0][\"cot\"]}')\n",
    "print(f'[Answer]\\n{examples[0][\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    final_answer: str = Field(description='A single numbers for final answer to the question')\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Answer)\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL'),\n",
    "    format='json'\n",
    ")\n",
    "\n",
    "system = '''You are an excellent math solver to help user. \n",
    "Please output only the final answer without any explanation. \n",
    "The format should be {{'final_answer': 'answer'}}.\n",
    "Please answer the following question. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: zero-shot prompt w/o chain of thoughts\n",
    "zero_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "keys = ['question']\n",
    "# output = chain.batch(inputs=[{k: q[k] for k in keys} for q in queries])\n",
    "output = chain.batch(inputs=[\n",
    "    zero_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[zero-shot prompt w/o CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    pprint(f'Question: {queries[i][\"question\"]}')\n",
    "    print(f'Answer: {queries[i][\"answer\"]}')\n",
    "    print(f'Predicted: {o[\"final_answer\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot prompt w/o chain of thoughts\n",
    "example_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('human', 'Question: {question}'),\n",
    "        ('ai', '{{\"final_answer\": {answer}}}'),\n",
    "    ]\n",
    ")\n",
    "keys = ['question', 'answer']\n",
    "few_shot_examples = FewShotChatMessagePromptTemplate(\n",
    "    examples=[{k: e[k] for k in keys} for e in examples],   # filter with keys\n",
    "    example_prompt=example_template\n",
    ")\n",
    "few_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        few_shot_examples,\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "keys = ['question']\n",
    "# output = chain.batch(inputs=[{k: q[k] for k in keys} for q in queries])\n",
    "output = chain.batch(inputs=[\n",
    "    few_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[few-shot prompt w/o CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    pprint(f'Question: {queries[i][\"question\"]}')\n",
    "    print(f'Answer: {queries[i][\"answer\"]}')\n",
    "    print(f'Predicted: {o[\"final_answer\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot prompt w/ chain of thoughts\n",
    "system = '''You are an excellent math solver to help user. \n",
    "The output format should be {{'chain_of_thought': 'step by step thinking process', 'final_answer': 'answer'}}.\n",
    "Please answer the following question. \n",
    "'''\n",
    "\n",
    "zero_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "keys = ['question']\n",
    "# output = chain.batch(inputs=[{k: q[k] for k in keys} for q in queries])\n",
    "output = chain.batch(inputs=[\n",
    "    zero_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[zero-shot prompt w/ CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    pprint(f'Question: {queries[i][\"question\"]}')\n",
    "    print(f'Answer: {queries[i][\"answer\"]}')\n",
    "    print(f'Predicted: {o[\"final_answer\"]}')\n",
    "    print(f'Chain of Thought: {o[\"chain_of_thought\"]}')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-app-sample-q-rYsYuW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
