{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikidocs.net/book/14314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 LangChain Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Options:\n",
    "* model_name(model): the name of the model\n",
    "* temperature: 0 ~ 2, larger is more random\n",
    "* max_tokens: depends on the model, the maximum number of tokens to generate\n",
    "* base_url: the base URL of the API\n",
    "* api_key: the API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Document: https://platform.openai.com/docs/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',  # latest model\n",
    "    temperature=0.2,\n",
    "    max_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Ollama - Open Source LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama local\n",
    "* model options: `~/.ollama/models` (Mac) or `/usr/share/ollama/.ollama/models` (Linux)\n",
    "* `gemma2:9b-instruct-q4_0`: 9 billion parameters with instruct tuning and 4-bit quantization\n",
    "* modelfile: A model file is the blueprint to create and share models with Ollama.\n",
    "     \n",
    "    ```shell\n",
    "    FROM $HOME/.ollama/models/blobs/sha256-aqoqkj13ik4ujn9oducnzkjdn\n",
    "    TEMPLATE \"<start_of_turn>user\n",
    "    {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }}<end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    {{ .Response }}<end_of_turn>\n",
    "    \"\n",
    "    PARAMETER stop <start_of_turn>\n",
    "    PARAMETER stop <end_of_turn>\n",
    "    ```\n",
    "    \n",
    "    * `PARAMETER`: \n",
    "        * temperature: 0 ~ 2, higher is more creative, lower is more coherent\n",
    "        * num_ctx: context window size\n",
    "    * `SYSTEM`: custom system message \n",
    "* Ollama CoLab\n",
    "* Ollama LangChain: must install `langhain-ollama` seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Ngrok URL: {os.getenv('NGROK_URL')}\")\n",
    "print(f\"LangChain Tracing: {os.getenv('LANGCHAIN_TRACING_V2')}\")\n",
    "print(f\"LangChain Project Name: {os.getenv('LANGCHAIN_PROJECT')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma:2b-instruct', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LECL: LangChain Expression Language "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain use `Runnable` interface to chain together a series of operations. Any two runnables can be 'chained' together into sequences. We can use `.pipe` method or `|` operator to chain operations together. The output of the previous runnable's `.invoke()` call is passed as input to the next runnable. \n",
    "\n",
    "* `invoke`: call the chain of operations\n",
    "* `stream`: stream the output of the chain of operations\n",
    "* `batch`: call the chain of operations with batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template('What is {topic}? Please answer in one sentence.')\n",
    "\n",
    "chain_openai = (prompt | model_openai | StrOutputParser())\n",
    "chain_pipe_ollama = (prompt.pipe(model_ollama).pipe(StrOutputParser()))\n",
    "\n",
    "print('Class Type of `chain` object:', type(chain_openai))\n",
    "print('# 1 `chain_openai`')\n",
    "print(chain_openai.invoke(input={'topic': '1 + 1'}))\n",
    "print('# 2 `chain_pipe_ollama`')\n",
    "print(chain_pipe_ollama.invoke(input={'topic': '1 + 1'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful `Runnable` classes:\n",
    "\n",
    "* `RunnablePassthrough` is a special runnable that passes the input to the output without any modification.\n",
    "    * if call `.assign` method, the output of the chain of operations is assigned to the variable.\n",
    "* `RunnableLambda` can be used to run a user-defined function on the input.\n",
    "* `RunnableParallel` is useful for running multiple runnables in parallel, but can also be useful for manipulating the output of one `Runnable` to match the input format of the next `Runnable` in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
    "\n",
    "set_debug(True)\n",
    "set_verbose(False)\n",
    "prompt = PromptTemplate.from_template('What is two words that start with {letter}?')\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | prompt\n",
    ")\n",
    "print('RunnablePassthrough:\\n  ', chain.invoke(input='A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough().assign(letter=lambda x: x['letter'].upper())\n",
    "    | prompt\n",
    ")\n",
    "print('RunnablePassthrough with assign:\\n  ', chain.invoke(input={'letter': 'a'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnableLambda(lambda x: x['letter'].upper()) \n",
    "    | prompt\n",
    ")\n",
    "print('RunnableLambda:\\n  ', chain.invoke(input={'letter': 'a'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_debug(False)\n",
    "set_verbose(False)\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "chain = RunnableParallel(\n",
    "    passed = RunnablePassthrough(),\n",
    "    extra = RunnableLambda(lambda x: x['letter'].upper()),\n",
    "    modefied = lambda x: random.choice(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\n",
    ")\n",
    "chain.invoke(input={'letter': 'a'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "chain1 = (\n",
    "    RunnablePassthrough()\n",
    "    | PromptTemplate.from_template('What is two words that start with {letter}?')\n",
    "    | model_openai\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain2 = (\n",
    "    RunnablePassthrough()\n",
    "    | PromptTemplate.from_template('What is two words that ends with {letter}?')\n",
    "    | model_openai\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain = RunnableParallel(\n",
    "    start=chain1,\n",
    "    end=chain2\n",
    ")\n",
    "output = chain.invoke(input={'letter': 'a'})\n",
    "print('Output:\\n')\n",
    "pprint(output)\n",
    "print('Graph:')\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "```python\n",
    "# travel information searching\n",
    "inputs = {'country': 'Korea', 'city': 'Jeonju'}\n",
    "# chain 1: find the two famous restaurant in the city\n",
    "# chain 2: find the three famous tourist spot in the city\n",
    "# chain 3: summarize the information in one paragraph chain 1 and chain 2\n",
    "```\n",
    "\n",
    "Total Graph\n",
    "\n",
    "```plaintext\n",
    "              +----------------------+               \n",
    "              | Parallel<c1,c2>Input |               \n",
    "              +----------------------+               \n",
    "                 **               **                 \n",
    "            **                         **            \n",
    "  +-------------+                  +-------------+   \n",
    "  | Passthrough |                  | Passthrough |   \n",
    "  +-------------+                  +-------------+   \n",
    "          *                               *          \n",
    "+----------------+                +----------------+ \n",
    "| PromptTemplate |                | PromptTemplate | \n",
    "+----------------+                +----------------+ \n",
    "          *                               *          \n",
    "  +------------+                    +------------+   \n",
    "  | ChatOllama |                    | ChatOllama |   \n",
    "  +------------+                    +------------+   \n",
    "          *                               *          \n",
    "+-----------------+              +-----------------+ \n",
    "| StrOutputParser |              | StrOutputParser | \n",
    "+-----------------+              +-----------------+ \n",
    "                 **               **                 \n",
    "                      **     **                      \n",
    "              +-----------------------+              \n",
    "              | Parallel<c1,c2>Output |              \n",
    "              +-----------------------+              \n",
    "                          *                          \n",
    "                 +----------------+                  \n",
    "                 | PromptTemplate |                  \n",
    "                 +----------------+                  \n",
    "                          *                          \n",
    "                   +------------+                    \n",
    "                   | ChatOllama |                    \n",
    "                   +------------+                    \n",
    "                          *                          \n",
    "                +-----------------+                  \n",
    "                | StrOutputParser |                  \n",
    "                +-----------------+                  \n",
    "                          *                          \n",
    "              +-----------------------+              \n",
    "              | StrOutputParserOutput |              \n",
    "              +-----------------------+    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'  # use 'false' to disable tracing if not needed\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',  # latest model\n",
    "    temperature=0.2,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL')\n",
    ")\n",
    "\n",
    "model = model_ollama\n",
    "chain1 = (\n",
    "    # a1: RunnablePassthrough or itemgetter\n",
    "    # a2: PromptTemplate.from_template(  ?  )\n",
    "    # | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "chain2 = (\n",
    "    # b1: RunnablePassthrough or itemgetter\n",
    "    # b2: PromptTemplate.from_template(  ?  )\n",
    "    # | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "chain = (\n",
    "    # RunnableParallel(c1=chain1, c2=chain2) \n",
    "    # c1: PromptTemplate.from_template(  ?  )\n",
    "    # | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "output = chain.invoke(input= {'country': 'Korea', 'city': 'Jeonju'})\n",
    "print('Output:\\n')\n",
    "print(output)\n",
    "print('Graph:')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **prompt** is natural language text describing the task that an AI should perform. It can contain information like the instruction or question passing to the model and include other details such as context, inputs, or examples. We can use these elements to instruct the model better and get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Prompt Template\n",
    "\n",
    "`PromptTemplate` helps to translate user input and parameters into instructions for a language model.\n",
    "* input: a dictionary of parameters\n",
    "* output: `PromptValue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplate\n",
    "from datetime import datetime as dt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = 'What is {country} capital city?'\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(template)\n",
    "prompt2 = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=['country']\n",
    ")\n",
    "prompt3 = PromptTemplate(\n",
    "    template='Date: {today}\\n' + template, \n",
    "    input_variables=['country'], \n",
    "    partial_variables={'today': dt.now().strftime('%Y-%m-%d')}  # predefined partial values\n",
    ")\n",
    "\n",
    "print('prompt1:', prompt1.invoke({'country': 'Korea'}))\n",
    "print('prompt2:', prompt2.invoke({'country': 'Korea'}))\n",
    "print('prompt3:', prompt3.invoke({'country': 'Korea'}))\n",
    "try:\n",
    "    prompt2.invoke({'c': 'Korea'})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate` is used to format a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant'),\n",
    "    ('user', 'Tell me a joke about {topic}')\n",
    "])\n",
    "prompt2 = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant'),\n",
    "    MessagesPlaceholder('msgs')\n",
    "])\n",
    "\n",
    "print('prompt1')\n",
    "print(prompt1.invoke({'topic': 'cats'}))\n",
    "print()\n",
    "print('prompt2')\n",
    "print(prompt2.invoke({'msgs': [HumanMessage(content='hi!')]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Few shot prompts\n",
    "\n",
    "One common prompting technique for achieving better performance is to include examples as part of the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL')\n",
    ")\n",
    "# translation: English to Korean\n",
    "examples = [\n",
    "    {'input': 'Hello', 'output': '안녕'},\n",
    "    {'input': 'Goodbye', 'output': '잘가'},\n",
    "    {'input': 'Thank you', 'output': '고마워'}\n",
    "]\n",
    "instruction = 'Translate english to Korean.'\n",
    "example_template = 'Input: {input} > Output: {output}'\n",
    "\n",
    "# zero-shot prompting\n",
    "prompt1 = PromptTemplate(\n",
    "    template=instruction+'\\n'+example_template, \n",
    "    input_variables=['input'], \n",
    "    partial_variables={'output': ''}\n",
    ")\n",
    "chain1 = (\n",
    "    prompt1\n",
    "    | model_ollama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# few-shot prompting\n",
    "prompt2 = FewShotPromptTemplate(\n",
    "    prefix=instruction,\n",
    "    examples=examples,\n",
    "    example_prompt=PromptTemplate.from_template(template=example_template),\n",
    "    suffix='Input: {input} > Output:',\n",
    "    input_variables=['input'],\n",
    ")\n",
    "chain2 = (\n",
    "    prompt2\n",
    "    | model_ollama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print('# Chain1\\n[Prompt]')\n",
    "print(prompt1.invoke(input='deep learning').text)\n",
    "print('[Output]')\n",
    "print(chain1.invoke(input='deep learning'))\n",
    "print('===='*10)\n",
    "print('# Chain2\\n[Prompt]')\n",
    "print(prompt2.invoke(input='deep learning').text)\n",
    "print('[Output]')\n",
    "print(chain2.invoke(input='deep learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n",
    "\n",
    "* Get format instructions: A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "* Parse: A method which takes in a string (assumed to be the response from a language model) and parses it into some structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL'),\n",
    "    num_ctx=500\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comma Sprerated List\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='List the five city of {country}.\\n{format_instructions}',\n",
    "    input_variables=['country'],\n",
    "    partial_variables={'format_instructions': output_parser.get_format_instructions()}\n",
    ")\n",
    "chain1 = prompt | model_ollama | output_parser\n",
    "chain2 = prompt | model_openai | output_parser\n",
    "\n",
    "print('[Prompt]')\n",
    "print(prompt.invoke(input={'country': 'Korea'}).text)\n",
    "print('[Output: Gemma2]')\n",
    "print(chain1.invoke(input={'country': 'Korea'}))  # gemma2 is too nice\n",
    "print('[Output: OpenAI]')\n",
    "print(chain2.invoke(input={'country': 'Korea'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json \n",
    "# - Pydantic: data validation library for Python\n",
    "from pprint import pprint\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class City(BaseModel):\n",
    "    name: str = Field(description='City name')\n",
    "    population: int = Field(description='City population')\n",
    "\n",
    "class CityList(BaseModel):\n",
    "    cities: list[City] = Field(description='List of cities ')\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=CityList)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='List two cities in {country} and tell me the population of these two cities.\\n{format_instructions}',\n",
    "    input_variables=['country'],\n",
    "    partial_variables={'format_instructions': output_parser.get_format_instructions()}\n",
    ")\n",
    "chain1 = prompt | model_ollama | output_parser\n",
    "chain2 = prompt | model_openai | output_parser\n",
    "\n",
    "print('[Prompt]')\n",
    "pprint(prompt.invoke(input={'country': 'Korea'}).text)\n",
    "print()\n",
    "print('[Output: Gemma2]')\n",
    "print(chain1.invoke(input={'country': 'Korea'}))\n",
    "print('[Output: OpenAI]')\n",
    "print(chain2.invoke(input={'country': 'Korea'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.with_structured_output()` method is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\n",
    "\n",
    "`WARNINGS`: only support for OpenAI for now.\n",
    "* support list: [https://python.langchain.com/v0.2/docs/integrations/chat/](https://python.langchain.com/v0.2/docs/integrations/chat/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output pydantic object\n",
    "class City(BaseModel):\n",
    "    name: str = Field(description='City name')\n",
    "    population: int = Field(description='City population')\n",
    "\n",
    "class CityList(BaseModel):\n",
    "    cities: list[City] = Field(description='List of cities ')\n",
    "\n",
    "llm = model_openai.with_structured_output(CityList)\n",
    "chain = prompt | llm\n",
    "output = chain.invoke(input={'country': 'Korea'})\n",
    "\n",
    "print(f'[Pydantic Object]: {type(output)}')\n",
    "print(output)\n",
    "\n",
    "# output Json object\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "class City2(TypedDict):\n",
    "    name: Annotated[str, ..., 'City name']\n",
    "    population: Annotated[int, ..., 'City population']\n",
    "\n",
    "class CityList2(TypedDict):\n",
    "    cities: Annotated[list[City2], 'List of cities']\n",
    "\n",
    "llm = model_openai.with_structured_output(CityList2)\n",
    "chain = prompt | llm\n",
    "output = chain.invoke(input={'country': 'Korea'})\n",
    "\n",
    "print(f'[Json Object]: {type(output)}')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model oriented format output\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b',\n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL'),\n",
    "    num_ctx=500,\n",
    "    format='json'\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    model_kwargs = {'response_format': {'type': 'json_object'}}\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='''List two cities in {country} and tell me the population of these two cities.\n",
    "Return the answer as a list of JSON object. e.g., [{{\"key1\": \"value1\", \"key2\": \"value2\"}}, ...] \n",
    "Each JSON object should have keys and their values: \n",
    "| \"name\" | \"type\": \"string\", \"description\": \"the name of the city\"\n",
    "| \"population\" | \"type\": \"integer\", \"description\": \"the population of the city\"\n",
    "''',\n",
    "    input_variables=['country'],\n",
    "    partial_variables={'schema': CityList.schema()}\n",
    ")\n",
    "chain1 = prompt | model_ollama\n",
    "chain2 = prompt | model_openai\n",
    "o1 = chain1.invoke(input={'country': 'Korea'})\n",
    "o2 = chain2.invoke(input={'country': 'Korea'})\n",
    "# the output of model is AI\n",
    "print(f'[Output: Gemma2] {type(o1.content)}')\n",
    "print(o1.content)\n",
    "print(f'[Output: OpenAI] {type(o2.content)}')\n",
    "print(o2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Chain of Thoughts Prompting\n",
    "\n",
    "Chain of thoughts prompting is a series of intermediate reasoning steps that guide the model to the final answer.\n",
    "\n",
    "`GSM8K (Grade School Math 8K)` is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(df: pd.DataFrame) -> list[dict[str, str]]:\n",
    "    df.rename(columns={'answer': 'cot_answer'}, inplace=True)\n",
    "    df['cot'] = df['cot_answer'].apply(lambda x: x.split('####')[0].strip())\n",
    "    df['answer'] = df['cot_answer'].apply(lambda x: x.split('####')[1].strip())\n",
    "    return df.to_dict(orient='records')\n",
    "\n",
    "df = pd.read_csv('GSM8K_test.csv')\n",
    "examples = df.loc[range(0, 5)].copy()\n",
    "queries = df.loc[range(5, 8)].copy()\n",
    "examples = process_data(examples)\n",
    "queries = process_data(queries)\n",
    "\n",
    "print(f'[Question]')\n",
    "for x in examples[0][\"question\"].split(\". \"):\n",
    "    print(x, end='.\\n')\n",
    "print(f'[Chain of Thought]\\n{examples[0][\"cot\"]}')\n",
    "print(f'[Answer]\\n{examples[0][\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    final_answer: str = Field(description='A single numbers for final answer to the question')\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Answer)\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0.3, \n",
    "    base_url=os.getenv('NGROK_URL'),\n",
    "    format='json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline: zero-shot prompt w/o chain of thoughts\n",
    "system = '''You are an excellent math solver to help user. \n",
    "Please output only the final answer without any explanation. \n",
    "The format should be {{'final_answer': 'answer'}}.\n",
    "Please answer the following question. \n",
    "'''\n",
    "\n",
    "zero_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = chain.batch(inputs=[\n",
    "    zero_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[zero-shot prompt w/o CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    print('[Question]')\n",
    "    pprint(f'{queries[i][\"question\"]}', width=120)\n",
    "    print(f'[Answer] {queries[i][\"answer\"]}')\n",
    "    print(f'[Predicted] {o[\"final_answer\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot prompt w/o chain of thoughts\n",
    "system = '''You are an excellent math solver to help user. \n",
    "Please output only the final answer without any explanation. \n",
    "The format should be {{'final_answer': 'answer'}}.\n",
    "Please answer the following question. \n",
    "'''\n",
    "\n",
    "example_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('human', 'Question: {question}'),\n",
    "        ('ai', '{{\"final_answer\": {answer}}}'),\n",
    "    ]\n",
    ")\n",
    "keys = ['question', 'answer']\n",
    "few_shot_examples = FewShotChatMessagePromptTemplate(\n",
    "    examples=[{k: e[k] for k in keys} for e in examples],   # filter with keys\n",
    "    example_prompt=example_template\n",
    ")\n",
    "few_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        few_shot_examples,\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = chain.batch(inputs=[\n",
    "    few_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[few-shot prompt w/o CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    print('[Question]')\n",
    "    pprint(f'{queries[i][\"question\"]}', width=120)\n",
    "    print(f'[Answer] {queries[i][\"answer\"]}')\n",
    "    print(f'[Predicted] {o[\"final_answer\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot prompt w/ chain of thoughts\n",
    "system = '''You are an excellent math solver to help user. \n",
    "The output format should be {{'chain_of_thought': 'step by step thinking process', 'final_answer': 'answer'}}.\n",
    "Please answer the following question. \n",
    "'''\n",
    "\n",
    "zero_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = chain.batch(inputs=[\n",
    "    zero_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[zero-shot prompt w CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    print('[Question]')\n",
    "    pprint(f'{queries[i][\"question\"]}', width=120)\n",
    "    print(f'[Answer] {queries[i][\"answer\"]}')\n",
    "    print(f'[Predicted] {o[\"final_answer\"]}')\n",
    "    print('[Predicted Chain of Thought]')\n",
    "    for c in o[\"chain_of_thought\"].split('\\n'):\n",
    "        print(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# few-shot prompt w/ chain of thoughts\n",
    "system = '''You are an excellent math solver to help user. \n",
    "The output format should be {{'chain_of_thought': 'step by step thinking process', 'final_answer': 'answer'}}.\n",
    "Please answer the following question. \n",
    "'''\n",
    "\n",
    "cot_example_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('human', 'Question: {question}'),\n",
    "        ('ai', '{{\"chain_of_thought\": \"{cot}\", \"final_answer\": \"{answer}\"}}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "keys = ['question', 'cot', 'answer']\n",
    "cot_few_shot_examples = FewShotChatMessagePromptTemplate(\n",
    "    examples=[{k: e[k] for k in keys} for e in examples],   # filter with keys\n",
    "    example_prompt=cot_example_template\n",
    ")\n",
    "\n",
    "cot_few_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        cot_few_shot_examples,\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = chain.batch(inputs=[\n",
    "    cot_few_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[few-shot prompt w/ CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    print('[Question]')\n",
    "    pprint(f'{queries[i][\"question\"]}', width=120)\n",
    "    print('[Chain of Thought]')\n",
    "    for c in queries[i][\"cot\"].split('\\n'):\n",
    "        print(c)\n",
    "    print(f'[Answer] {queries[i][\"answer\"]}')\n",
    "    print(f'[Predicted] {o[\"final_answer\"]}')\n",
    "    print(f'[Predicted Chain of Thought]')\n",
    "    for c in o[\"chain_of_thought\"].split('\\n'):\n",
    "        print(c)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-app-sample-q-rYsYuW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
