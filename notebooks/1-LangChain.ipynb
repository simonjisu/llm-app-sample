{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://wikidocs.net/book/14314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 LangChain Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Large Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM Options:\n",
    "* model_name(model): the name of the model\n",
    "* temperature: 0 ~ 2, larger is more random\n",
    "* max_tokens: depends on the model, the maximum number of tokens to generate\n",
    "* base_url: the base URL of the API\n",
    "* api_key: the API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Document: https://platform.openai.com/docs/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',  # latest model\n",
    "    temperature=0.2,\n",
    "    max_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Ollama - Open Source LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama local\n",
    "* model options: `~/.ollama/models` (Mac) or `/usr/share/ollama/.ollama/models` (Linux)\n",
    "* `gemma2:9b-instruct-q4_0`: 9 billion parameters with instruct tuning and 4-bit quantization\n",
    "* modelfile: A model file is the blueprint to create and share models with Ollama.\n",
    "     \n",
    "    ```shell\n",
    "    FROM $HOME/.ollama/models/blobs/sha256-aqoqkj13ik4ujn9oducnzkjdn\n",
    "    TEMPLATE \"<start_of_turn>user\n",
    "    {{ if .System }}{{ .System }} {{ end }}{{ .Prompt }}<end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    {{ .Response }}<end_of_turn>\n",
    "    \"\n",
    "    PARAMETER stop <start_of_turn>\n",
    "    PARAMETER stop <end_of_turn>\n",
    "    ```\n",
    "    \n",
    "    * `PARAMETER`: \n",
    "        * temperature: 0 ~ 2, higher is more creative, lower is more coherent\n",
    "        * num_ctx: context window size\n",
    "    * `SYSTEM`: custom system message \n",
    "* Ollama CoLab\n",
    "* Ollama LangChain: must install `langhain-ollama` seperately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngrok URL: https://square-sunfish-vaguely.ngrok-free.app\n",
      "LangChain Tracing: false\n",
      "LangChain Project Name: sample_app\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(f\"Ngrok URL: {os.getenv('NGROK_URL')}\")\n",
    "print(f\"LangChain Tracing: {os.getenv('LANGCHAIN_TRACING_V2')}\")\n",
    "print(f\"LangChain Project Name: {os.getenv('LANGCHAIN_PROJECT')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma:2b-instruct', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LECL: LangChain Expression Language "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain use `Runnable` interface to chain together a series of operations. Any two runnables can be 'chained' together into sequences. We can use `.pipe` method or `|` operator to chain operations together. The output of the previous runnable's `.invoke()` call is passed as input to the next runnable. \n",
    "\n",
    "* `invoke`: call the chain of operations\n",
    "* `stream`: stream the output of the chain of operations\n",
    "* `batch`: call the chain of operations with batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Type of `chain` object: <class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "# 1 `chain_openai`\n",
      "1 + 1 equals 2.\n",
      "# 2 `chain_pipe_ollama`\n",
      "1 + 1 = 2.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template('What is {topic}? Please answer in one sentence.')\n",
    "\n",
    "chain_openai = (prompt | model_openai | StrOutputParser())\n",
    "chain_pipe_ollama = (prompt.pipe(model_ollama).pipe(StrOutputParser()))\n",
    "\n",
    "print('Class Type of `chain` object:', type(chain_openai))\n",
    "print('# 1 `chain_openai`')\n",
    "print(chain_openai.invoke(input={'topic': '1 + 1'}))\n",
    "print('# 2 `chain_pipe_ollama`')\n",
    "print(chain_pipe_ollama.invoke(input={'topic': '1 + 1'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful `Runnable` classes:\n",
    "\n",
    "* `RunnablePassthrough` is a special runnable that passes the input to the output without any modification.\n",
    "    * if call `.assign` method, the output of the chain of operations is assigned to the variable.\n",
    "* `RunnableLambda` can be used to run a user-defined function on the input.\n",
    "* `RunnableParallel` is useful for running multiple runnables in parallel, but can also be useful for manipulating the output of one `Runnable` to match the input format of the next `Runnable` in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"A\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"A\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "RunnablePassthrough:\n",
      "   text='What is two words that start with A?'\n"
     ]
    }
   ],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda, RunnableParallel\n",
    "\n",
    "set_debug(True)\n",
    "set_verbose(False)\n",
    "prompt = PromptTemplate.from_template('What is two words that start with {letter}?')\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | prompt\n",
    ")\n",
    "print('RunnablePassthrough:\\n  ', chain.invoke(input='A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter> > chain:RunnableParallel<letter>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter> > chain:RunnableParallel<letter> > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter> > chain:RunnableParallel<letter> > chain:RunnableLambda] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter> > chain:RunnableParallel<letter>] [3ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableAssign<letter>] [7ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"A\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [14ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "RunnablePassthrough with assign:\n",
      "   text='What is two words that start with A?'\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough().assign(letter=lambda x: x['letter'].upper())\n",
    "    | prompt\n",
    ")\n",
    "print('RunnablePassthrough with assign:\\n  ', chain.invoke(input={'letter': 'a'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"letter\": \"a\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableLambda] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"A\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"A\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [2ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "RunnableLambda:\n",
      "   text='What is two words that start with A?'\n"
     ]
    }
   ],
   "source": [
    "chain = (\n",
    "    RunnableLambda(lambda x: x['letter'].upper()) \n",
    "    | prompt\n",
    ")\n",
    "print('RunnableLambda:\\n  ', chain.invoke(input={'letter': 'a'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-------------+      \n",
      "    | LambdaInput |      \n",
      "    +-------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "       +--------+        \n",
      "       | Lambda |        \n",
      "       +--------+        \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "   +----------------+    \n",
      "   | PromptTemplate |    \n",
      "   +----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "+----------------------+ \n",
      "| PromptTemplateOutput | \n",
      "+----------------------+ \n"
     ]
    }
   ],
   "source": [
    "set_debug(False)\n",
    "set_verbose(False)\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'passed': {'letter': 'a'}, 'extra': 'A', 'modefied': 'K'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "chain = RunnableParallel(\n",
    "    passed = RunnablePassthrough(),\n",
    "    extra = RunnableLambda(lambda x: x['letter'].upper()),\n",
    "    modefied = lambda x: random.choice(list('ABCDEFGHIJKLMNOPQRSTUVWXYZ'))\n",
    ")\n",
    "chain.invoke(input={'letter': 'a'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "\n",
      "{'end': 'Here are two words that end with the letter \"a\": \\n'\n",
      "        '\\n'\n",
      "        '1. Arena\\n'\n",
      "        '2. Panda',\n",
      " 'start': 'Here are two words that start with the letter \"A\": \\n'\n",
      "          '\\n'\n",
      "          '1. Apple\\n'\n",
      "          '2. Adventure'}\n",
      "Graph:\n",
      "            +--------------------------+             \n",
      "            | Parallel<start,end>Input |             \n",
      "            +--------------------------+             \n",
      "                 **               **                 \n",
      "              ***                   ***              \n",
      "            **                         **            \n",
      "  +-------------+                  +-------------+   \n",
      "  | Passthrough |                  | Passthrough |   \n",
      "  +-------------+                  +-------------+   \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "+----------------+                +----------------+ \n",
      "| PromptTemplate |                | PromptTemplate | \n",
      "+----------------+                +----------------+ \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "  +------------+                    +------------+   \n",
      "  | ChatOpenAI |                    | ChatOpenAI |   \n",
      "  +------------+                    +------------+   \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "          *                               *          \n",
      "+-----------------+              +-----------------+ \n",
      "| StrOutputParser |              | StrOutputParser | \n",
      "+-----------------+              +-----------------+ \n",
      "                 **               **                 \n",
      "                   ***         ***                   \n",
      "                      **     **                      \n",
      "            +---------------------------+            \n",
      "            | Parallel<start,end>Output |            \n",
      "            +---------------------------+            \n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "chain1 = (\n",
    "    RunnablePassthrough()\n",
    "    | PromptTemplate.from_template('What is two words that start with {letter}?')\n",
    "    | model_openai\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain2 = (\n",
    "    RunnablePassthrough()\n",
    "    | PromptTemplate.from_template('What is two words that ends with {letter}?')\n",
    "    | model_openai\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain = RunnableParallel(\n",
    "    start=chain1,\n",
    "    end=chain2\n",
    ")\n",
    "output = chain.invoke(input={'letter': 'a'})\n",
    "print('Output:\\n')\n",
    "pprint(output)\n",
    "print('Graph:')\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "```python\n",
    "# travel information searching\n",
    "inputs = {'country': 'Korea', 'city': 'Jeonju'}\n",
    "# chain 1: find the two famous restaurant in the city\n",
    "# chain 2: find the three famous tourist spot in the city\n",
    "# chain 3: summarize the information in one paragraph chain 1 and chain 2\n",
    "```\n",
    "\n",
    "Total Graph\n",
    "\n",
    "```plaintext\n",
    "              +----------------------+               \n",
    "              | Parallel<c1,c2>Input |               \n",
    "              +----------------------+               \n",
    "                 **               **                 \n",
    "            **                         **            \n",
    "  +-------------+                  +-------------+   \n",
    "  | Passthrough |                  | Passthrough |   \n",
    "  +-------------+                  +-------------+   \n",
    "          *                               *          \n",
    "+----------------+                +----------------+ \n",
    "| PromptTemplate |                | PromptTemplate | \n",
    "+----------------+                +----------------+ \n",
    "          *                               *          \n",
    "  +------------+                    +------------+   \n",
    "  | ChatOllama |                    | ChatOllama |   \n",
    "  +------------+                    +------------+   \n",
    "          *                               *          \n",
    "+-----------------+              +-----------------+ \n",
    "| StrOutputParser |              | StrOutputParser | \n",
    "+-----------------+              +-----------------+ \n",
    "                 **               **                 \n",
    "                      **     **                      \n",
    "              +-----------------------+              \n",
    "              | Parallel<c1,c2>Output |              \n",
    "              +-----------------------+              \n",
    "                          *                          \n",
    "                 +----------------+                  \n",
    "                 | PromptTemplate |                  \n",
    "                 +----------------+                  \n",
    "                          *                          \n",
    "                   +------------+                    \n",
    "                   | ChatOllama |                    \n",
    "                   +------------+                    \n",
    "                          *                          \n",
    "                +-----------------+                  \n",
    "                | StrOutputParser |                  \n",
    "                +-----------------+                  \n",
    "                          *                          \n",
    "              +-----------------------+              \n",
    "              | StrOutputParserOutput |              \n",
    "              +-----------------------+    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'  # use 'false' to disable tracing if not needed\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',  # latest model\n",
    "    temperature=0.2,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL')\n",
    ")\n",
    "\n",
    "model = model_ollama\n",
    "chain1 = (\n",
    "    # a1: RunnablePassthrough or itemgetter\n",
    "    # a2: PromptTemplate.from_template(  ?  )\n",
    "    # | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "chain2 = (\n",
    "    # b1: RunnablePassthrough or itemgetter\n",
    "    # b2: PromptTemplate.from_template(  ?  )\n",
    "    # | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "chain = (\n",
    "    # RunnableParallel(c1=chain1, c2=chain2) \n",
    "    # c1: PromptTemplate.from_template(  ?  )\n",
    "    # | model\n",
    "    # | StrOutputParser()\n",
    ")\n",
    "output = chain.invoke(input= {'country': 'Korea', 'city': 'Jeonju'})\n",
    "print('Output:\\n')\n",
    "print(output)\n",
    "print('Graph:')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'false'\n",
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **prompt** is natural language text describing the task that an AI should perform. It can contain information like the instruction or question passing to the model and include other details such as context, inputs, or examples. We can use these elements to instruct the model better and get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Prompt Template\n",
    "\n",
    "`PromptTemplate` helps to translate user input and parameters into instructions for a language model.\n",
    "* input: a dictionary of parameters\n",
    "* output: `PromptValue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt1: text='What is Korea capital city?'\n",
      "prompt2: text='What is Korea capital city?'\n",
      "prompt3: text='Date: 2024-08-05\\nWhat is Korea capital city?'\n",
      "\"Input to PromptTemplate is missing variables {'country'}.  Expected: ['country'] Received: ['c']\"\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate\n",
    "from datetime import datetime as dt\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = 'What is {country} capital city?'\n",
    "\n",
    "prompt1 = PromptTemplate.from_template(template)\n",
    "prompt2 = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=['country']\n",
    ")\n",
    "prompt3 = PromptTemplate(\n",
    "    template='Date: {today}\\n' + template, \n",
    "    input_variables=['country'], \n",
    "    partial_variables={'today': dt.now().strftime('%Y-%m-%d')}  # predefined partial values\n",
    ")\n",
    "\n",
    "print('prompt1:', prompt1.invoke({'country': 'Korea'}))\n",
    "print('prompt2:', prompt2.invoke({'country': 'Korea'}))\n",
    "print('prompt3:', prompt3.invoke({'country': 'Korea'}))\n",
    "try:\n",
    "    prompt2.invoke({'c': 'Korea'})\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ChatPromptTemplate` is used to format a list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt1\n",
      "messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='Tell me a joke about cats')]\n",
      "\n",
      "prompt2\n",
      "messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='hi!')]\n"
     ]
    }
   ],
   "source": [
    "# ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant'),\n",
    "    ('user', 'Tell me a joke about {topic}')\n",
    "])\n",
    "prompt2 = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'You are a helpful assistant'),\n",
    "    MessagesPlaceholder('msgs')\n",
    "])\n",
    "\n",
    "print('prompt1')\n",
    "print(prompt1.invoke({'topic': 'cats'}))\n",
    "print()\n",
    "print('prompt2')\n",
    "print(prompt2.invoke({'msgs': [HumanMessage(content='hi!')]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Few shot prompts\n",
    "\n",
    "One common prompting technique for achieving better performance is to include examples as part of the prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Chain1\n",
      "[Prompt]\n",
      "Translate english to Korean.\n",
      "Input: deep learning > Output: \n",
      "[Output]\n",
      "깊은 학습  (kireun haksaep) \n",
      "\n",
      "\n",
      "Let me know if you have any other phrases you'd like translated! 😊 \n",
      "\n",
      "========================================\n",
      "# Chain2\n",
      "[Prompt]\n",
      "Translate english to Korean.\n",
      "\n",
      "Input: Hello > Output: 안녕\n",
      "\n",
      "Input: Goodbye > Output: 잘가\n",
      "\n",
      "Input: Thank you > Output: 고마워\n",
      "\n",
      "Input: deep learning > Output:\n",
      "[Output]\n",
      "Input: deep learning > Output: 딥러닝 \n",
      "\n",
      "\n",
      "Let me know if you'd like to translate anything else! 😊  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL')\n",
    ")\n",
    "# translation: English to Korean\n",
    "examples = [\n",
    "    {'input': 'Hello', 'output': '안녕'},\n",
    "    {'input': 'Goodbye', 'output': '잘가'},\n",
    "    {'input': 'Thank you', 'output': '고마워'}\n",
    "]\n",
    "instruction = 'Translate english to Korean.'\n",
    "example_template = 'Input: {input} > Output: {output}'\n",
    "\n",
    "# zero-shot prompting\n",
    "prompt1 = PromptTemplate(\n",
    "    template=instruction+'\\n'+example_template, \n",
    "    input_variables=['input'], \n",
    "    partial_variables={'output': ''}\n",
    ")\n",
    "chain1 = (\n",
    "    prompt1\n",
    "    | model_ollama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# few-shot prompting\n",
    "prompt2 = FewShotPromptTemplate(\n",
    "    prefix=instruction,\n",
    "    examples=examples,\n",
    "    example_prompt=PromptTemplate.from_template(template=example_template),\n",
    "    suffix='Input: {input} > Output:',\n",
    "    input_variables=['input'],\n",
    ")\n",
    "chain2 = (\n",
    "    prompt2\n",
    "    | model_ollama\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print('# Chain1\\n[Prompt]')\n",
    "print(prompt1.invoke(input='deep learning').text)\n",
    "print('[Output]')\n",
    "print(chain1.invoke(input='deep learning'))\n",
    "print('===='*10)\n",
    "print('# Chain2\\n[Prompt]')\n",
    "print(prompt2.invoke(input='deep learning').text)\n",
    "print('[Output]')\n",
    "print(chain2.invoke(input='deep learning'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output parsers are classes that help structure language model responses. There are two main methods an output parser must implement:\n",
    "\n",
    "* Get format instructions: A method which returns a string containing instructions for how the output of a language model should be formatted.\n",
    "* Parse: A method which takes in a string (assumed to be the response from a language model) and parses it into some structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser, JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL'),\n",
    "    num_ctx=500\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt]\n",
      "List the five city of Korea.\n",
      "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n",
      "[Output: Gemma2]\n",
      "['Seoul', 'Busan', 'Daegu', 'Incheon', \"Gwangju \\n\\n\\nLet me know if you'd like to know more about these cities! 😊\"]\n",
      "[Output: OpenAI]\n",
      "['Seoul', 'Busan', 'Incheon', 'Daegu', 'Gwangju']\n"
     ]
    }
   ],
   "source": [
    "# Comma Sprerated List\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='List the five city of {country}.\\n{format_instructions}',\n",
    "    input_variables=['country'],\n",
    "    partial_variables={'format_instructions': output_parser.get_format_instructions()}\n",
    ")\n",
    "chain1 = prompt | model_ollama | output_parser\n",
    "chain2 = prompt | model_openai | output_parser\n",
    "\n",
    "print('[Prompt]')\n",
    "print(prompt.invoke(input={'country': 'Korea'}).text)\n",
    "print('[Output: Gemma2]')\n",
    "print(chain1.invoke(input={'country': 'Korea'}))  # gemma2 is too nice\n",
    "print('[Output: OpenAI]')\n",
    "print(chain2.invoke(input={'country': 'Korea'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt]\n",
      "('List two cities in Korea and tell me the population of these two cities.\\n'\n",
      " 'The output should be formatted as a JSON instance that conforms to the JSON '\n",
      " 'schema below.\\n'\n",
      " '\\n'\n",
      " 'As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", '\n",
      " '\"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": '\n",
      " '\"string\"}}}, \"required\": [\"foo\"]}\\n'\n",
      " 'the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the '\n",
      " 'schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not '\n",
      " 'well-formatted.\\n'\n",
      " '\\n'\n",
      " 'Here is the output schema:\\n'\n",
      " '```\\n'\n",
      " '{\"properties\": {\"cities\": {\"title\": \"Cities\", \"description\": \"List of cities '\n",
      " '\", \"type\": \"array\", \"items\": {\"$ref\": \"#/definitions/City\"}}}, \"required\": '\n",
      " '[\"cities\"], \"definitions\": {\"City\": {\"title\": \"City\", \"type\": \"object\", '\n",
      " '\"properties\": {\"name\": {\"title\": \"Name\", \"description\": \"City name\", \"type\": '\n",
      " '\"string\"}, \"population\": {\"title\": \"Population\", \"description\": \"City '\n",
      " 'population\", \"type\": \"integer\"}}, \"required\": [\"name\", \"population\"]}}}\\n'\n",
      " '```')\n",
      "\n",
      "[Output: Gemma2]\n",
      "{'cities': [{'name': 'Seoul', 'population': 9600000}, {'name': 'Busan', 'population': 3400000}]}\n",
      "[Output: OpenAI]\n",
      "{'cities': [{'name': 'Seoul', 'population': 9776000}, {'name': 'Busan', 'population': 3406000}]}\n"
     ]
    }
   ],
   "source": [
    "# Json \n",
    "# - Pydantic: data validation library for Python\n",
    "from pprint import pprint\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class City(BaseModel):\n",
    "    name: str = Field(description='City name')\n",
    "    population: int = Field(description='City population')\n",
    "\n",
    "class CityList(BaseModel):\n",
    "    cities: list[City] = Field(description='List of cities ')\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=CityList)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='List two cities in {country} and tell me the population of these two cities.\\n{format_instructions}',\n",
    "    input_variables=['country'],\n",
    "    partial_variables={'format_instructions': output_parser.get_format_instructions()}\n",
    ")\n",
    "chain1 = prompt | model_ollama | output_parser\n",
    "chain2 = prompt | model_openai | output_parser\n",
    "\n",
    "print('[Prompt]')\n",
    "pprint(prompt.invoke(input={'country': 'Korea'}).text)\n",
    "print()\n",
    "print('[Output: Gemma2]')\n",
    "print(chain1.invoke(input={'country': 'Korea'}))\n",
    "print('[Output: OpenAI]')\n",
    "print(chain2.invoke(input={'country': 'Korea'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.with_structured_output()` method is implemented for models that provide native APIs for structuring outputs, like tool/function calling or JSON mode, and makes use of these capabilities under the hood.\n",
    "\n",
    "`WARNINGS`: only support for OpenAI for now.\n",
    "* support list: [https://python.langchain.com/v0.2/docs/integrations/chat/](https://python.langchain.com/v0.2/docs/integrations/chat/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pydantic Object]: <class '__main__.CityList'>\n",
      "cities=[City(name='Seoul', population=9776000), City(name='Busan', population=3406000)]\n",
      "[Json Object]: <class 'dict'>\n",
      "{'cities': [{'name': 'Seoul', 'population': 9776000}, {'name': 'Busan', 'population': 3406000}]}\n"
     ]
    }
   ],
   "source": [
    "# output pydantic object\n",
    "class City(BaseModel):\n",
    "    name: str = Field(description='City name')\n",
    "    population: int = Field(description='City population')\n",
    "\n",
    "class CityList(BaseModel):\n",
    "    cities: list[City] = Field(description='List of cities ')\n",
    "\n",
    "llm = model_openai.with_structured_output(CityList)\n",
    "chain = prompt | llm\n",
    "output = chain.invoke(input={'country': 'Korea'})\n",
    "\n",
    "print(f'[Pydantic Object]: {type(output)}')\n",
    "print(output)\n",
    "\n",
    "# output Json object\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "class City2(TypedDict):\n",
    "    name: Annotated[str, ..., 'City name']\n",
    "    population: Annotated[int, ..., 'City population']\n",
    "\n",
    "class CityList2(TypedDict):\n",
    "    cities: Annotated[list[City2], 'List of cities']\n",
    "\n",
    "llm = model_openai.with_structured_output(CityList2)\n",
    "chain = prompt | llm\n",
    "output = chain.invoke(input={'country': 'Korea'})\n",
    "\n",
    "print(f'[Json Object]: {type(output)}')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Output: Gemma2] <class 'str'>\n",
      "{\n",
      "  \"cities\": [\n",
      "    {\n",
      "      \"name\": \"Seoul\",\n",
      "      \"population\": 9605000\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Busan\",\n",
      "      \"population\": 3400000\n",
      "    }\n",
      "  ]\n",
      "} \n",
      "\n",
      "\n",
      "\n",
      "[Output: OpenAI] <class 'str'>\n",
      "\n",
      "{\n",
      "  \"cities\": [\n",
      "    {\n",
      "      \"name\": \"Seoul\",\n",
      "      \"population\": 9776000\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Busan\",\n",
      "      \"population\": 3406000\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# model oriented format output\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b',\n",
    "    temperature=0, \n",
    "    base_url=os.getenv('NGROK_URL'),\n",
    "    num_ctx=500,\n",
    "    format='json'\n",
    ")\n",
    "\n",
    "model_openai = ChatOpenAI(\n",
    "    model='gpt-4o-mini',\n",
    "    temperature=0,\n",
    "    max_tokens=500,\n",
    "    model_kwargs = {'response_format': {'type': 'json_object'}}\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template='''List two cities in {country} and tell me the population of these two cities.\n",
    "Return the answer as a list of JSON object. e.g., [{{\"key1\": \"value1\", \"key2\": \"value2\"}}, ...] \n",
    "Each JSON object should have keys and their values: \n",
    "| \"name\" | \"type\": \"string\", \"description\": \"the name of the city\"\n",
    "| \"population\" | \"type\": \"integer\", \"description\": \"the population of the city\"\n",
    "''',\n",
    "    input_variables=['country'],\n",
    "    partial_variables={'schema': CityList.schema()}\n",
    ")\n",
    "chain1 = prompt | model_ollama\n",
    "chain2 = prompt | model_openai\n",
    "o1 = chain1.invoke(input={'country': 'Korea'})\n",
    "o2 = chain2.invoke(input={'country': 'Korea'})\n",
    "# the output of model is AI\n",
    "print(f'[Output: Gemma2] {type(o1.content)}')\n",
    "print(o1.content)\n",
    "print(f'[Output: OpenAI] {type(o2.content)}')\n",
    "print(o2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Chain of Thoughts Prompting\n",
    "\n",
    "Chain of thoughts prompting is a series of intermediate reasoning steps that guide the model to the final answer.\n",
    "\n",
    "`GSM8K (Grade School Math 8K)` is a dataset of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Question]\n",
      "Janet’s ducks lay 16 eggs per day.\n",
      "She eats three for breakfast every morning and bakes muffins for her friends every day with four.\n",
      "She sells the remainder at the farmers' market daily for $2 per fresh duck egg.\n",
      "How much in dollars does she make every day at the farmers' market?.\n",
      "[Chain of Thought]\n",
      "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "[Answer]\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def process_data(df: pd.DataFrame) -> list[dict[str, str]]:\n",
    "    df.rename(columns={'answer': 'cot_answer'}, inplace=True)\n",
    "    df['cot'] = df['cot_answer'].apply(lambda x: x.split('####')[0].strip())\n",
    "    df['answer'] = df['cot_answer'].apply(lambda x: x.split('####')[1].strip())\n",
    "    return df.to_dict(orient='records')\n",
    "\n",
    "df = pd.read_csv('GSM8K_test.csv')\n",
    "examples = df.loc[range(0, 5)].copy()\n",
    "queries = df.loc[range(5, 8)].copy()\n",
    "examples = process_data(examples)\n",
    "queries = process_data(queries)\n",
    "\n",
    "print(f'[Question]')\n",
    "for x in examples[0][\"question\"].split(\". \"):\n",
    "    print(x, end='.\\n')\n",
    "print(f'[Chain of Thought]\\n{examples[0][\"cot\"]}')\n",
    "print(f'[Answer]\\n{examples[0][\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Answer(BaseModel):\n",
    "    final_answer: str = Field(description='A single numbers for final answer to the question')\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Answer)\n",
    "\n",
    "model_ollama = ChatOllama(\n",
    "    model='gemma2:9b', \n",
    "    temperature=0.3, \n",
    "    base_url=os.getenv('NGROK_URL'),\n",
    "    format='json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[zero-shot prompt w/o CoT]\n",
      "[Question]\n",
      "('Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only '\n",
      " '60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?')\n",
      "[Answer] 64\n",
      "[Predicted] 72\n",
      "\n",
      "[Question]\n",
      "('Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How many sheep do '\n",
      " 'Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep?')\n",
      "[Answer] 260\n",
      "[Predicted] 160\n",
      "\n",
      "[Question]\n",
      "('Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but 40% of the way through the download, '\n",
      " 'Windows forces a restart to install updates, which takes 20 minutes. Then Carla has to restart the download from the '\n",
      " 'beginning. How load does it take to download the file?')\n",
      "[Answer] 160\n",
      "[Predicted] 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baseline: zero-shot prompt w/o chain of thoughts\n",
    "system = '''You are an excellent math solver to help user. \n",
    "Please output only the final answer without any explanation. \n",
    "The format should be {{'final_answer': 'answer'}}.\n",
    "Please answer the following question. \n",
    "'''\n",
    "\n",
    "zero_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = chain.batch(inputs=[\n",
    "    zero_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[zero-shot prompt w/o CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    print('[Question]')\n",
    "    pprint(f'{queries[i][\"question\"]}', width=120)\n",
    "    print(f'[Answer] {queries[i][\"answer\"]}')\n",
    "    print(f'[Predicted] {o[\"final_answer\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[few-shot prompt w/o CoT]\n",
      "[Question]\n",
      "('Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only '\n",
      " '60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?')\n",
      "[Answer] 64\n",
      "[Predicted] 72\n",
      "\n",
      "[Question]\n",
      "('Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How many sheep do '\n",
      " 'Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep?')\n",
      "[Answer] 260\n",
      "[Predicted] 160\n",
      "\n",
      "[Question]\n",
      "('Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but 40% of the way through the download, '\n",
      " 'Windows forces a restart to install updates, which takes 20 minutes. Then Carla has to restart the download from the '\n",
      " 'beginning. How load does it take to download the file?')\n",
      "[Answer] 160\n",
      "[Predicted] 160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# few-shot prompt w/o chain of thoughts\n",
    "system = '''You are an excellent math solver to help user. \n",
    "Please output only the final answer without any explanation. \n",
    "The format should be {{'final_answer': 'answer'}}.\n",
    "Please answer the following question. \n",
    "'''\n",
    "\n",
    "example_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('human', 'Question: {question}'),\n",
    "        ('ai', '{{\"final_answer\": {answer}}}'),\n",
    "    ]\n",
    ")\n",
    "keys = ['question', 'answer']\n",
    "few_shot_examples = FewShotChatMessagePromptTemplate(\n",
    "    examples=[{k: e[k] for k in keys} for e in examples],   # filter with keys\n",
    "    example_prompt=example_template\n",
    ")\n",
    "few_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        few_shot_examples,\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = chain.batch(inputs=[\n",
    "    few_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[few-shot prompt w/o CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    print('[Question]')\n",
    "    pprint(f'{queries[i][\"question\"]}', width=120)\n",
    "    print(f'[Answer] {queries[i][\"answer\"]}')\n",
    "    print(f'[Predicted] {o[\"final_answer\"]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[zero-shot prompt w CoT]\n",
      "[Question]\n",
      "('Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only '\n",
      " '60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?')\n",
      "[Answer] 64\n",
      "[Predicted] $50\n",
      "[Predicted Chain of Thought]\n",
      "Here's how to solve the problem: \n",
      "\n",
      "* **Calculate the cost of the second and subsequent glasses:**  60% of $5 is (60/100) * $5 = $3.\n",
      "* **Determine the number of full-priced and discounted glasses:** Kylar needs 16 glasses, so he'll buy 15 glasses at the discounted price and 1 at the full price.\n",
      "* **Calculate the total cost of the discounted glasses:** 15 glasses * $3/glass = $45\n",
      "* **Calculate the total cost:** $45 + $5 = $50\n",
      "\n",
      "[Question]\n",
      "('Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How many sheep do '\n",
      " 'Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep?')\n",
      "[Answer] 260\n",
      "[Predicted] 260\n",
      "[Predicted Chain of Thought]\n",
      "1. **Charleston's sheep:** Charleston has 4 times as many sheep as Seattle, which has 20 sheep. So Charleston has 4 * 20 = 80 sheep.\n",
      "2. **Toulouse's sheep:** Toulouse has twice as many sheep as Charleston, who has 80 sheep. Therefore, Toulouse has 2 * 80 = 160 sheep.\n",
      "3. **Total sheep:** To find the total number of sheep, add the number of sheep in each city: 20 (Seattle) + 80 (Charleston) + 160 (Toulouse) = 260 sheep.\n",
      "\n",
      "[Question]\n",
      "('Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but 40% of the way through the download, '\n",
      " 'Windows forces a restart to install updates, which takes 20 minutes. Then Carla has to restart the download from the '\n",
      " 'beginning. How load does it take to download the file?')\n",
      "[Answer] 160\n",
      "[Predicted] 180 minutes\n",
      "[Predicted Chain of Thought]\n",
      "Here's how we can solve this problem step-by-step: \n",
      "1. **Calculate the download time for the first half:** 200 GB / 2 GB/minute = 100 minutes\n",
      "2. **Calculate the data downloaded before the restart:** 200 GB * 40% = 80 GB\n",
      "3. **Calculate the remaining data to download after the restart:** 200 GB - 80 GB = 120 GB\n",
      "4. **Calculate the time to download the remaining data:** 120 GB / 2 GB/minute = 60 minutes\n",
      "5. **Add the time for the restart and the total download time:** 100 minutes + 20 minutes + 60 minutes = 180 minutes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# zero-shot prompt w/ chain of thoughts\n",
    "system = '''You are an excellent math solver to help user. \n",
    "The output format should be {{'chain_of_thought': 'step by step thinking process', 'final_answer': 'answer'}}.\n",
    "Please answer the following question. \n",
    "'''\n",
    "\n",
    "zero_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = chain.batch(inputs=[\n",
    "    zero_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[zero-shot prompt w CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    print('[Question]')\n",
    "    pprint(f'{queries[i][\"question\"]}', width=120)\n",
    "    print(f'[Answer] {queries[i][\"answer\"]}')\n",
    "    print(f'[Predicted] {o[\"final_answer\"]}')\n",
    "    print('[Predicted Chain of Thought]')\n",
    "    for c in o[\"chain_of_thought\"].split('\\n'):\n",
    "        print(c)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[few-shot prompt w/ CoT]\n",
      "[Question]\n",
      "('Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only '\n",
      " '60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?')\n",
      "[Chain of Thought]\n",
      "The discount price of one glass is 60/100 * 5 = $<<60/100*5=3>>3.\n",
      "If every second glass is cheaper, that means Kylar is going to buy 16 / 2 = <<16/2=8>>8 cheaper glasses.\n",
      "So for the cheaper glasses, Kylar is going to pay 8 * 3 = $<<8*3=24>>24.\n",
      "And for the regular-priced glasses, Kylar will pay 8 * 5 = $<<8*5=40>>40.\n",
      "So in total Kylar needs to pay 24 + 40 = $<<24+40=64>>64 for the glasses he wants to buy.\n",
      "[Answer] 64\n",
      "[Predicted] 64\n",
      "[Predicted Chain of Thought]\n",
      "Every other glass is discounted, so we have 16 / 2 = <<16/2=8>>8 full-priced glasses and 8 discounted glasses. \n",
      "\n",
      "The cost of the full-priced glasses is 8 * $5 = <<8*$5=$40>>$40.\n",
      "\n",
      "The cost of the discounted glasses is 8 * ($5 * 0.60) = <<8*$5*0.60=$24>>$24.\n",
      "\n",
      "The total cost is $40 + $24 = <<$40+$24=$64>>$64.\n",
      "\n",
      "[Question]\n",
      "('Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How many sheep do '\n",
      " 'Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep?')\n",
      "[Chain of Thought]\n",
      "If Seattle has 20 sheep, Charleston has 4 * 20 sheep = <<20*4=80>>80 sheep\n",
      "Toulouse has twice as many sheep as Charleston, which is 2 * 80 sheep = <<2*80=160>>160 sheep\n",
      "Together, the three has 20 sheep + 160 sheep + 80 sheep = <<20+160+80=260>>260 sheep\n",
      "[Answer] 260\n",
      "[Predicted] 260\n",
      "[Predicted Chain of Thought]\n",
      "Charleston has 4*20=<<4*20=80>>80 sheep.  Toulouse has 2*80=<<2*80=160>>160 sheep. Together they have 160+80+20=<<160+80+20=260>>260 sheep.\n",
      "\n",
      "[Question]\n",
      "('Carla is downloading a 200 GB file. Normally she can download 2 GB/minute, but 40% of the way through the download, '\n",
      " 'Windows forces a restart to install updates, which takes 20 minutes. Then Carla has to restart the download from the '\n",
      " 'beginning. How load does it take to download the file?')\n",
      "[Chain of Thought]\n",
      "First find how many gigabytes are in 40% of the file: 200 GB * 40% = <<200*40*.01=80>>80 GB\n",
      "Then divide that number by the download rate to find the time until Windows restarts: 80 GB / 2 GB/minute = <<80/2=40>>40 minutes\n",
      "Then find the time to download the whole file after the restart: 200 GB / 2 GB/minute = <<200/2=100>>100 minutes\n",
      "Then add the time to download 40% of the file, to download the whole file, and to wait for Windows to update: 40 minutes + 100 minutes + 20 minutes = <<40+100+20=160>>160 minutes\n",
      "[Answer] 160\n",
      "[Predicted] 160\n",
      "[Predicted Chain of Thought]\n",
      "Carla downloads 200 * 0.4 = <<200*0.4=80>>80 GB before the restart.  This takes 80 / 2 = <<80/2=40>>40 minutes. After the restart, she has to download another 200 GB, which takes 200 / 2 = <<200/2=100>>100 minutes. The total time is 40 + 20 + 100 = <<40+20+100=160>>160 minutes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# few-shot prompt w/ chain of thoughts\n",
    "system = '''You are an excellent math solver to help user. \n",
    "The output format should be {{'chain_of_thought': 'step by step thinking process', 'final_answer': 'answer'}}.\n",
    "Please answer the following question. \n",
    "'''\n",
    "\n",
    "cot_example_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('human', 'Question: {question}'),\n",
    "        ('ai', '{{\"chain_of_thought\": \"{cot}\", \"final_answer\": \"{answer}\"}}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "keys = ['question', 'cot', 'answer']\n",
    "cot_few_shot_examples = FewShotChatMessagePromptTemplate(\n",
    "    examples=[{k: e[k] for k in keys} for e in examples],   # filter with keys\n",
    "    example_prompt=cot_example_template\n",
    ")\n",
    "\n",
    "cot_few_shot_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system),\n",
    "        cot_few_shot_examples,\n",
    "        ('human', 'Question: {question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | model_ollama\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "output = chain.batch(inputs=[\n",
    "    cot_few_shot_template.format_messages(question=q['question']) for q in queries\n",
    "])\n",
    "\n",
    "print('[few-shot prompt w/ CoT]')\n",
    "for i, o in enumerate(output):\n",
    "    print('[Question]')\n",
    "    pprint(f'{queries[i][\"question\"]}', width=120)\n",
    "    print('[Chain of Thought]')\n",
    "    for c in queries[i][\"cot\"].split('\\n'):\n",
    "        print(c)\n",
    "    print(f'[Answer] {queries[i][\"answer\"]}')\n",
    "    print(f'[Predicted] {o[\"final_answer\"]}')\n",
    "    print(f'[Predicted Chain of Thought]')\n",
    "    for c in o[\"chain_of_thought\"].split('\\n'):\n",
    "        print(c)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-app-sample-q-rYsYuW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
